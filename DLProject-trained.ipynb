{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:38:23.165229: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 15:38:23.214268: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/bmi-lab/Downloads/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3'\n",
    "\n",
    "ptbxl_df = pd.read_csv(os.path.join(DATA_PATH, 'ptbxl_database.csv'))\n",
    "scp_statements = pd.read_csv(os.path.join(DATA_PATH, 'scp_statements.csv'), index_col=0)\n",
    "\n",
    "diagnostic_scps = scp_statements[scp_statements['diagnostic'] == 1].index.values\n",
    "\n",
    "scp_to_superclass = scp_statements['diagnostic_class'].to_dict()\n",
    "scp_to_subclass = scp_statements['diagnostic_subclass'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_df['scp_codes'] = ptbxl_df['scp_codes'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_diagnostic_labels(df, scp_codes, scp_to_agg):\n",
    "    df = df.copy()\n",
    "    def aggregate_labels(scp_codes_dict):\n",
    "        labels = set()\n",
    "        for code in scp_codes_dict.keys():\n",
    "            if code in scp_codes:\n",
    "                label = scp_to_agg.get(code)\n",
    "                if label:\n",
    "                    labels.add(label)\n",
    "        return list(labels)\n",
    "    df['diagnostic_labels'] = df['scp_codes'].apply(aggregate_labels)\n",
    "    return df\n",
    "\n",
    "ptbxl_df = aggregate_diagnostic_labels(ptbxl_df, diagnostic_scps, scp_to_superclass)\n",
    "ptbxl_df = ptbxl_df.rename(columns={'diagnostic_labels': 'superclass_labels'})\n",
    "\n",
    "ptbxl_df = aggregate_diagnostic_labels(ptbxl_df, diagnostic_scps, scp_to_subclass)\n",
    "ptbxl_df = ptbxl_df.rename(columns={'diagnostic_labels': 'subclass_labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_df = ptbxl_df[ptbxl_df['superclass_labels'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ptbxl_df[ptbxl_df.strat_fold <= 8]\n",
    "val_df = ptbxl_df[ptbxl_df.strat_fold == 9]\n",
    "test_df = ptbxl_df[ptbxl_df.strat_fold == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, sampling_rate, data_path):\n",
    "    data = []\n",
    "    if sampling_rate == 100:\n",
    "        filenames = df['filename_lr'].values\n",
    "    else:\n",
    "        filenames = df['filename_hr'].values\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        signals, _ = wfdb.rdsamp(file_path)\n",
    "        data.append(signals)\n",
    "    return np.array(data)\n",
    "\n",
    "X_train = load_data(train_df, sampling_rate=100, data_path=DATA_PATH)\n",
    "X_val = load_data(val_df, sampling_rate=100, data_path=DATA_PATH)\n",
    "X_test = load_data(test_df, sampling_rate=100, data_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_super = train_df['superclass_labels'].values\n",
    "val_labels_super = val_df['superclass_labels'].values\n",
    "test_labels_super = test_df['superclass_labels'].values\n",
    "\n",
    "mlb_super = MultiLabelBinarizer()\n",
    "y_train_super = mlb_super.fit_transform(train_labels_super)\n",
    "y_val_super = mlb_super.transform(val_labels_super)\n",
    "y_test_super = mlb_super.transform(test_labels_super)\n",
    "classes_super = mlb_super.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_sub = train_df['subclass_labels'].values\n",
    "val_labels_sub = val_df['subclass_labels'].values\n",
    "test_labels_sub = test_df['subclass_labels'].values\n",
    "\n",
    "mlb_sub = MultiLabelBinarizer()\n",
    "y_train_sub = mlb_sub.fit_transform(train_labels_sub)\n",
    "y_val_sub = mlb_sub.transform(val_labels_sub)\n",
    "y_test_sub = mlb_sub.transform(test_labels_sub)\n",
    "classes_sub = mlb_sub.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_per_channel(X):\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "    mean = np.mean(X, axis=(0, 2), keepdims=True)\n",
    "    std = np.std(X, axis=(0, 2), keepdims=True)\n",
    "    X = (X - mean) / std\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "    return X\n",
    "\n",
    "X_train = normalize_data_per_channel(X_train)\n",
    "X_val = normalize_data_per_channel(X_val)\n",
    "X_test = normalize_data_per_channel(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_super = np.sum(y_train_super, axis=0)\n",
    "total_samples_super = y_train_super.shape[0]\n",
    "\n",
    "class_weight_super = {}\n",
    "for i, count in enumerate(class_counts_super):\n",
    "    class_weight_super[i] = total_samples_super / (len(class_counts_super) * count)\n",
    "\n",
    "class_counts_sub = np.sum(y_train_sub, axis=0)\n",
    "total_samples_sub = y_train_sub.shape[0]\n",
    "\n",
    "class_weight_sub = {}\n",
    "for i, count in enumerate(class_counts_sub):\n",
    "    class_weight_sub[i] = total_samples_sub / (len(class_counts_sub) * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_super = y_train_super.shape[1]\n",
    "class_totals = np.sum(y_train_super, axis=0)\n",
    "class_weights = class_totals.max() / class_totals\n",
    "weights_array = np.array(class_weights, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_sub = y_train_sub.shape[1]\n",
    "class_totals_sub = np.sum(y_train_sub, axis=0)\n",
    "class_weights_sub = class_totals_sub.max() / class_totals_sub\n",
    "weights_array_sub = np.array(class_weights_sub, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_super = y_train_super.astype(np.float32)\n",
    "y_val_super = y_val_super.astype(np.float32)\n",
    "y_test_super = y_test_super.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Entropy and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def weighted_binary_crossentropy(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        weights_cast = K.cast(weights, y_pred.dtype)\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "        \n",
    "        bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        weight_vector = y_true * weights_cast + (1 - y_true)\n",
    "        weighted_bce = weight_vector * bce\n",
    "        return K.mean(weighted_bce)\n",
    "    return loss\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred = K.round(y_pred)\n",
    "    \n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, kernel_size=7, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, kernel_size=5, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_resnet_model(input_shape, num_classes):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x = layers.Conv1D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "#     previous_filters = x.shape[-1]\n",
    "#     for filters in [64, 128, 256]:\n",
    "#         x_shortcut = x\n",
    "#         strides = 1\n",
    "#         if previous_filters != filters:\n",
    "#             strides = 2\n",
    "\n",
    "#         x = layers.Conv1D(filters, kernel_size=3, strides=strides, padding='same')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.Activation('relu')(x)\n",
    "#         x = layers.Conv1D(filters, kernel_size=3, padding='same')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        \n",
    "#         if previous_filters != filters or strides != 1:\n",
    "#             x_shortcut = layers.Conv1D(filters, kernel_size=1, strides=strides, padding='same')(x_shortcut)\n",
    "#             x_shortcut = layers.BatchNormalization()(x_shortcut)\n",
    "        \n",
    "#         x = layers.Add()([x, x_shortcut])\n",
    "#         x = layers.Activation('relu')(x)\n",
    "#         previous_filters = filters\n",
    "#     x = layers.GlobalAveragePooling1D()(x)\n",
    "#     outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block_1d(x, filters, kernel_size=3, strides=1, downsample=False):\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.Conv1D(filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, kernel_size=1, strides=strides, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def create_resnet_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    layers_filters = [64, 128, 256, 512]\n",
    "    layers_blocks = [3, 4, 6, 3]\n",
    "\n",
    "    for filters, num_blocks in zip(layers_filters, layers_blocks):\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0 and filters != x.shape[-1]:\n",
    "                x = residual_block_1d(x, filters, strides=2, downsample=True)\n",
    "            else:\n",
    "                x = residual_block_1d(x, filters)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def create_vit_model(input_shape, num_classes):\n",
    "    patch_size = 10 \n",
    "    num_patches = input_shape[0] // patch_size\n",
    "    projection_dim = 64\n",
    "    num_heads = 4\n",
    "    transformer_layers = 8\n",
    "    mlp_head_units = [256, 128]\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Reshape((num_patches, patch_size * input_shape[1]))(inputs)\n",
    "    x = layers.Dense(units=projection_dim)(x)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "    x = x + position_embedding(positions)\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=[projection_dim * 2, projection_dim], dropout_rate=dropout_rate)\n",
    "        x = layers.Add()([x3, x2])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, class_weight, batch_size=64, epochs=50):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', macro_f1]\n",
    "    )\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Models without CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:38:44.219509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.236045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.238077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.240180: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 15:38:44.241054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.242638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.244181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.334866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.336103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.337216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 15:38:44.338481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13716 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:38:45.723645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2024-11-21 15:38:45.769927: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-21 15:38:45.853407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-21 15:38:45.865045: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x781be003c160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-21 15:38:45.865074: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 4080, Compute Capability 8.9\n",
      "2024-11-21 15:38:45.868120: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-21 15:38:45.903369: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-21 15:38:45.914456: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/267 [==============================] - 4s 7ms/step - loss: 0.2743 - accuracy: 0.6490 - macro_f1: 0.6678 - val_loss: 0.3357 - val_accuracy: 0.6822 - val_macro_f1: 0.6863 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2378 - accuracy: 0.6889 - macro_f1: 0.7137 - val_loss: 0.4414 - val_accuracy: 0.5634 - val_macro_f1: 0.6120 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2233 - accuracy: 0.7046 - macro_f1: 0.7331 - val_loss: 0.3249 - val_accuracy: 0.6850 - val_macro_f1: 0.6811 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2157 - accuracy: 0.7137 - macro_f1: 0.7406 - val_loss: 0.3031 - val_accuracy: 0.6897 - val_macro_f1: 0.7017 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2086 - accuracy: 0.7200 - macro_f1: 0.7519 - val_loss: 0.2786 - val_accuracy: 0.7078 - val_macro_f1: 0.7244 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2023 - accuracy: 0.7248 - macro_f1: 0.7601 - val_loss: 0.3047 - val_accuracy: 0.6985 - val_macro_f1: 0.6986 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1951 - accuracy: 0.7327 - macro_f1: 0.7710 - val_loss: 0.2939 - val_accuracy: 0.7111 - val_macro_f1: 0.6977 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1921 - accuracy: 0.7314 - macro_f1: 0.7698 - val_loss: 0.3076 - val_accuracy: 0.6859 - val_macro_f1: 0.7117 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1844 - accuracy: 0.7435 - macro_f1: 0.7811 - val_loss: 0.3128 - val_accuracy: 0.6650 - val_macro_f1: 0.7137 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1792 - accuracy: 0.7468 - macro_f1: 0.7880 - val_loss: 0.2891 - val_accuracy: 0.6915 - val_macro_f1: 0.7229 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1507 - accuracy: 0.7676 - macro_f1: 0.8236 - val_loss: 0.2721 - val_accuracy: 0.7060 - val_macro_f1: 0.7318 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1420 - accuracy: 0.7766 - macro_f1: 0.8345 - val_loss: 0.2738 - val_accuracy: 0.7097 - val_macro_f1: 0.7308 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1374 - accuracy: 0.7833 - macro_f1: 0.8394 - val_loss: 0.2785 - val_accuracy: 0.7027 - val_macro_f1: 0.7297 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1339 - accuracy: 0.7841 - macro_f1: 0.8448 - val_loss: 0.2849 - val_accuracy: 0.7013 - val_macro_f1: 0.7298 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1303 - accuracy: 0.7848 - macro_f1: 0.8489 - val_loss: 0.2854 - val_accuracy: 0.7055 - val_macro_f1: 0.7302 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.1258 - accuracy: 0.7898 - macro_f1: 0.8542 - val_loss: 0.2877 - val_accuracy: 0.6976 - val_macro_f1: 0.7304 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1201 - accuracy: 0.7913 - macro_f1: 0.8635 - val_loss: 0.2876 - val_accuracy: 0.7078 - val_macro_f1: 0.7299 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.1187 - accuracy: 0.7968 - macro_f1: 0.8625 - val_loss: 0.2893 - val_accuracy: 0.7060 - val_macro_f1: 0.7283 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1171 - accuracy: 0.7958 - macro_f1: 0.8640 - val_loss: 0.2905 - val_accuracy: 0.7102 - val_macro_f1: 0.7282 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1173 - accuracy: 0.7971 - macro_f1: 0.8662 - val_loss: 0.2913 - val_accuracy: 0.7083 - val_macro_f1: 0.7280 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1165 - accuracy: 0.7988 - macro_f1: 0.8670 - val_loss: 0.2915 - val_accuracy: 0.7074 - val_macro_f1: 0.7296 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781e9323b400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes_super = y_train_super.shape[1]\n",
    "\n",
    "cnn_super_model = create_cnn_model(input_shape, num_classes_super)\n",
    "train_model(cnn_super_model, X_train, y_train_super, X_val, y_val_super, class_weight_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 14s 22ms/step - loss: 0.3035 - accuracy: 0.6257 - macro_f1: 0.6251 - val_loss: 0.4720 - val_accuracy: 0.6137 - val_macro_f1: 0.5847 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2563 - accuracy: 0.6764 - macro_f1: 0.6912 - val_loss: 0.3942 - val_accuracy: 0.6705 - val_macro_f1: 0.6552 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2403 - accuracy: 0.6963 - macro_f1: 0.7120 - val_loss: 0.4370 - val_accuracy: 0.5718 - val_macro_f1: 0.6332 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2306 - accuracy: 0.7013 - macro_f1: 0.7265 - val_loss: 0.4077 - val_accuracy: 0.6654 - val_macro_f1: 0.6687 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2239 - accuracy: 0.7093 - macro_f1: 0.7327 - val_loss: 0.3650 - val_accuracy: 0.6538 - val_macro_f1: 0.6768 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2176 - accuracy: 0.7161 - macro_f1: 0.7431 - val_loss: 0.3566 - val_accuracy: 0.6575 - val_macro_f1: 0.6808 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2118 - accuracy: 0.7178 - macro_f1: 0.7482 - val_loss: 0.3750 - val_accuracy: 0.6948 - val_macro_f1: 0.6772 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2052 - accuracy: 0.7224 - macro_f1: 0.7552 - val_loss: 0.3262 - val_accuracy: 0.6952 - val_macro_f1: 0.7054 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2007 - accuracy: 0.7300 - macro_f1: 0.7623 - val_loss: 0.3216 - val_accuracy: 0.6897 - val_macro_f1: 0.6957 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1950 - accuracy: 0.7375 - macro_f1: 0.7700 - val_loss: 0.3652 - val_accuracy: 0.7064 - val_macro_f1: 0.6877 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1915 - accuracy: 0.7395 - macro_f1: 0.7745 - val_loss: 0.3787 - val_accuracy: 0.6617 - val_macro_f1: 0.7017 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1865 - accuracy: 0.7422 - macro_f1: 0.7789 - val_loss: 0.3223 - val_accuracy: 0.6696 - val_macro_f1: 0.6919 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1803 - accuracy: 0.7480 - macro_f1: 0.7889 - val_loss: 0.5128 - val_accuracy: 0.6556 - val_macro_f1: 0.6812 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1702 - accuracy: 0.7519 - macro_f1: 0.7982 - val_loss: 0.3637 - val_accuracy: 0.6729 - val_macro_f1: 0.6705 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1346 - accuracy: 0.7851 - macro_f1: 0.8438 - val_loss: 0.3010 - val_accuracy: 0.7069 - val_macro_f1: 0.7127 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1158 - accuracy: 0.7999 - macro_f1: 0.8652 - val_loss: 0.3353 - val_accuracy: 0.6813 - val_macro_f1: 0.7085 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1004 - accuracy: 0.8050 - macro_f1: 0.8825 - val_loss: 0.3656 - val_accuracy: 0.6990 - val_macro_f1: 0.7011 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0881 - accuracy: 0.8150 - macro_f1: 0.8990 - val_loss: 0.4078 - val_accuracy: 0.6887 - val_macro_f1: 0.7030 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0744 - accuracy: 0.8194 - macro_f1: 0.9136 - val_loss: 0.4443 - val_accuracy: 0.6864 - val_macro_f1: 0.7010 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0608 - accuracy: 0.8284 - macro_f1: 0.9303 - val_loss: 0.5390 - val_accuracy: 0.6799 - val_macro_f1: 0.6915 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0439 - accuracy: 0.8401 - macro_f1: 0.9516 - val_loss: 0.5316 - val_accuracy: 0.6808 - val_macro_f1: 0.7007 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0389 - accuracy: 0.8377 - macro_f1: 0.9579 - val_loss: 0.5773 - val_accuracy: 0.6761 - val_macro_f1: 0.6959 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0350 - accuracy: 0.8410 - macro_f1: 0.9605 - val_loss: 0.6256 - val_accuracy: 0.6724 - val_macro_f1: 0.6958 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0332 - accuracy: 0.8415 - macro_f1: 0.9640 - val_loss: 0.6546 - val_accuracy: 0.6752 - val_macro_f1: 0.6945 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0311 - accuracy: 0.8410 - macro_f1: 0.9667 - val_loss: 0.7080 - val_accuracy: 0.6692 - val_macro_f1: 0.6916 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781e7dd44a00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_super_model = create_resnet_model(input_shape, num_classes_super)\n",
    "train_model(resnet_super_model, X_train, y_train_super, X_val, y_val_super, class_weight_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 13s 21ms/step - loss: 0.3845 - accuracy: 0.4978 - macro_f1: 0.4749 - val_loss: 0.3900 - val_accuracy: 0.5760 - val_macro_f1: 0.5902 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2904 - accuracy: 0.6254 - macro_f1: 0.6316 - val_loss: 0.3478 - val_accuracy: 0.6184 - val_macro_f1: 0.6666 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2591 - accuracy: 0.6665 - macro_f1: 0.6806 - val_loss: 0.3423 - val_accuracy: 0.6230 - val_macro_f1: 0.6588 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2377 - accuracy: 0.6829 - macro_f1: 0.7115 - val_loss: 0.3319 - val_accuracy: 0.6673 - val_macro_f1: 0.6525 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2196 - accuracy: 0.7082 - macro_f1: 0.7362 - val_loss: 0.3439 - val_accuracy: 0.6547 - val_macro_f1: 0.6629 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2122 - accuracy: 0.7120 - macro_f1: 0.7456 - val_loss: 0.3435 - val_accuracy: 0.6547 - val_macro_f1: 0.6811 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1959 - accuracy: 0.7235 - macro_f1: 0.7672 - val_loss: 0.3522 - val_accuracy: 0.6552 - val_macro_f1: 0.6753 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1825 - accuracy: 0.7360 - macro_f1: 0.7853 - val_loss: 0.3536 - val_accuracy: 0.6789 - val_macro_f1: 0.6717 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1699 - accuracy: 0.7456 - macro_f1: 0.7974 - val_loss: 0.3613 - val_accuracy: 0.6556 - val_macro_f1: 0.6786 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1226 - accuracy: 0.7859 - macro_f1: 0.8618 - val_loss: 0.3955 - val_accuracy: 0.6705 - val_macro_f1: 0.6738 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1085 - accuracy: 0.7988 - macro_f1: 0.8797 - val_loss: 0.4098 - val_accuracy: 0.6654 - val_macro_f1: 0.6694 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1003 - accuracy: 0.8094 - macro_f1: 0.8905 - val_loss: 0.4274 - val_accuracy: 0.6645 - val_macro_f1: 0.6733 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0936 - accuracy: 0.8099 - macro_f1: 0.8957 - val_loss: 0.4327 - val_accuracy: 0.6673 - val_macro_f1: 0.6730 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0887 - accuracy: 0.8181 - macro_f1: 0.9008 - val_loss: 0.4468 - val_accuracy: 0.6636 - val_macro_f1: 0.6716 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781e20affbb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_super_model = create_vit_model(input_shape, num_classes_super)\n",
    "train_model(vit_super_model, X_train, y_train_super, X_val, y_val_super, class_weight_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 3s 7ms/step - loss: 0.1060 - accuracy: 0.4069 - macro_f1: 0.1361 - val_loss: 0.1573 - val_accuracy: 0.4664 - val_macro_f1: 0.1466 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0841 - accuracy: 0.4723 - macro_f1: 0.2053 - val_loss: 0.1556 - val_accuracy: 0.4101 - val_macro_f1: 0.1858 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0758 - accuracy: 0.5023 - macro_f1: 0.2463 - val_loss: 0.1330 - val_accuracy: 0.4972 - val_macro_f1: 0.2232 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0690 - accuracy: 0.5283 - macro_f1: 0.2697 - val_loss: 0.1353 - val_accuracy: 0.4856 - val_macro_f1: 0.2309 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0642 - accuracy: 0.5364 - macro_f1: 0.2887 - val_loss: 0.1394 - val_accuracy: 0.4744 - val_macro_f1: 0.2182 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0596 - accuracy: 0.5564 - macro_f1: 0.3148 - val_loss: 0.1163 - val_accuracy: 0.5755 - val_macro_f1: 0.2707 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0557 - accuracy: 0.5747 - macro_f1: 0.3218 - val_loss: 0.1189 - val_accuracy: 0.5531 - val_macro_f1: 0.2763 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0564 - accuracy: 0.5745 - macro_f1: 0.3314 - val_loss: 0.1144 - val_accuracy: 0.5680 - val_macro_f1: 0.2705 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0515 - accuracy: 0.5890 - macro_f1: 0.3502 - val_loss: 0.1260 - val_accuracy: 0.5037 - val_macro_f1: 0.2759 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0485 - accuracy: 0.6010 - macro_f1: 0.3635 - val_loss: 0.1129 - val_accuracy: 0.5857 - val_macro_f1: 0.3071 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0462 - accuracy: 0.6049 - macro_f1: 0.3737 - val_loss: 0.1189 - val_accuracy: 0.5452 - val_macro_f1: 0.3058 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0456 - accuracy: 0.6134 - macro_f1: 0.3788 - val_loss: 0.1223 - val_accuracy: 0.5219 - val_macro_f1: 0.3166 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0437 - accuracy: 0.6170 - macro_f1: 0.3894 - val_loss: 0.1181 - val_accuracy: 0.5447 - val_macro_f1: 0.2914 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0407 - accuracy: 0.6244 - macro_f1: 0.4055 - val_loss: 0.1092 - val_accuracy: 0.5727 - val_macro_f1: 0.3167 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0385 - accuracy: 0.6353 - macro_f1: 0.4150 - val_loss: 0.1114 - val_accuracy: 0.5839 - val_macro_f1: 0.3186 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0368 - accuracy: 0.6434 - macro_f1: 0.4264 - val_loss: 0.1378 - val_accuracy: 0.5182 - val_macro_f1: 0.3012 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0385 - accuracy: 0.6312 - macro_f1: 0.4227 - val_loss: 0.1229 - val_accuracy: 0.5466 - val_macro_f1: 0.3005 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0349 - accuracy: 0.6437 - macro_f1: 0.4404 - val_loss: 0.1174 - val_accuracy: 0.5573 - val_macro_f1: 0.3016 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0315 - accuracy: 0.6579 - macro_f1: 0.4599 - val_loss: 0.1177 - val_accuracy: 0.5904 - val_macro_f1: 0.3295 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0253 - accuracy: 0.6908 - macro_f1: 0.5052 - val_loss: 0.1075 - val_accuracy: 0.6095 - val_macro_f1: 0.3319 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0233 - accuracy: 0.6982 - macro_f1: 0.5204 - val_loss: 0.1088 - val_accuracy: 0.6030 - val_macro_f1: 0.3367 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0224 - accuracy: 0.7036 - macro_f1: 0.5276 - val_loss: 0.1103 - val_accuracy: 0.6058 - val_macro_f1: 0.3369 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0216 - accuracy: 0.7066 - macro_f1: 0.5321 - val_loss: 0.1113 - val_accuracy: 0.6016 - val_macro_f1: 0.3320 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0211 - accuracy: 0.7083 - macro_f1: 0.5328 - val_loss: 0.1111 - val_accuracy: 0.6104 - val_macro_f1: 0.3302 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0205 - accuracy: 0.7124 - macro_f1: 0.5342 - val_loss: 0.1140 - val_accuracy: 0.6095 - val_macro_f1: 0.3333 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0196 - accuracy: 0.7162 - macro_f1: 0.5492 - val_loss: 0.1123 - val_accuracy: 0.6100 - val_macro_f1: 0.3397 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0195 - accuracy: 0.7187 - macro_f1: 0.5441 - val_loss: 0.1120 - val_accuracy: 0.6123 - val_macro_f1: 0.3370 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0194 - accuracy: 0.7174 - macro_f1: 0.5448 - val_loss: 0.1121 - val_accuracy: 0.6142 - val_macro_f1: 0.3378 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0193 - accuracy: 0.7178 - macro_f1: 0.5509 - val_loss: 0.1121 - val_accuracy: 0.6123 - val_macro_f1: 0.3383 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0194 - accuracy: 0.7161 - macro_f1: 0.5459 - val_loss: 0.1126 - val_accuracy: 0.6137 - val_macro_f1: 0.3373 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781de219ab30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_sub = y_train_sub.shape[1]\n",
    "cnn_sub_model = create_cnn_model(input_shape, num_classes_sub)\n",
    "train_model(cnn_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 14s 21ms/step - loss: 0.1160 - accuracy: 0.3864 - macro_f1: 0.0962 - val_loss: 0.2165 - val_accuracy: 0.1887 - val_macro_f1: 0.0947 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0970 - accuracy: 0.4214 - macro_f1: 0.1343 - val_loss: 0.2236 - val_accuracy: 0.2111 - val_macro_f1: 0.0981 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0877 - accuracy: 0.4701 - macro_f1: 0.1673 - val_loss: 0.2312 - val_accuracy: 0.1137 - val_macro_f1: 0.0978 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0822 - accuracy: 0.4720 - macro_f1: 0.1954 - val_loss: 0.1804 - val_accuracy: 0.3458 - val_macro_f1: 0.1432 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0810 - accuracy: 0.4797 - macro_f1: 0.2065 - val_loss: 0.9378 - val_accuracy: 0.0061 - val_macro_f1: 0.0037 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0811 - accuracy: 0.4775 - macro_f1: 0.1979 - val_loss: 0.1575 - val_accuracy: 0.4287 - val_macro_f1: 0.1426 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0729 - accuracy: 0.5105 - macro_f1: 0.2395 - val_loss: 0.1877 - val_accuracy: 0.1859 - val_macro_f1: 0.1233 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0689 - accuracy: 0.5108 - macro_f1: 0.2579 - val_loss: 0.1334 - val_accuracy: 0.4991 - val_macro_f1: 0.2169 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0654 - accuracy: 0.5269 - macro_f1: 0.2753 - val_loss: 0.1474 - val_accuracy: 0.4627 - val_macro_f1: 0.2065 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0624 - accuracy: 0.5417 - macro_f1: 0.2942 - val_loss: 0.1293 - val_accuracy: 0.5284 - val_macro_f1: 0.2284 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0601 - accuracy: 0.5488 - macro_f1: 0.3072 - val_loss: 0.1578 - val_accuracy: 0.5238 - val_macro_f1: 0.2358 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0614 - accuracy: 0.5475 - macro_f1: 0.3026 - val_loss: 0.1997 - val_accuracy: 0.2120 - val_macro_f1: 0.1659 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0689 - accuracy: 0.5081 - macro_f1: 0.2753 - val_loss: 0.2127 - val_accuracy: 0.4240 - val_macro_f1: 0.1945 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0617 - accuracy: 0.5410 - macro_f1: 0.3005 - val_loss: 0.1341 - val_accuracy: 0.4846 - val_macro_f1: 0.2501 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0535 - accuracy: 0.5690 - macro_f1: 0.3316 - val_loss: 0.1202 - val_accuracy: 0.5652 - val_macro_f1: 0.2606 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0543 - accuracy: 0.5660 - macro_f1: 0.3277 - val_loss: 0.1259 - val_accuracy: 0.5051 - val_macro_f1: 0.2741 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0494 - accuracy: 0.5854 - macro_f1: 0.3580 - val_loss: 0.1225 - val_accuracy: 0.5391 - val_macro_f1: 0.2930 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0470 - accuracy: 0.5957 - macro_f1: 0.3633 - val_loss: 0.1136 - val_accuracy: 0.5629 - val_macro_f1: 0.2944 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0457 - accuracy: 0.6017 - macro_f1: 0.3736 - val_loss: 0.1725 - val_accuracy: 0.3369 - val_macro_f1: 0.2434 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0541 - accuracy: 0.5581 - macro_f1: 0.3396 - val_loss: 0.1291 - val_accuracy: 0.4902 - val_macro_f1: 0.2843 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0442 - accuracy: 0.6014 - macro_f1: 0.3815 - val_loss: 0.1185 - val_accuracy: 0.5322 - val_macro_f1: 0.3056 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0402 - accuracy: 0.6151 - macro_f1: 0.4075 - val_loss: 0.1226 - val_accuracy: 0.5452 - val_macro_f1: 0.3100 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0374 - accuracy: 0.6301 - macro_f1: 0.4205 - val_loss: 0.1299 - val_accuracy: 0.5121 - val_macro_f1: 0.2854 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0301 - accuracy: 0.6594 - macro_f1: 0.4644 - val_loss: 0.1126 - val_accuracy: 0.5909 - val_macro_f1: 0.3346 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0269 - accuracy: 0.6715 - macro_f1: 0.4898 - val_loss: 0.1161 - val_accuracy: 0.5871 - val_macro_f1: 0.3320 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0252 - accuracy: 0.6786 - macro_f1: 0.5006 - val_loss: 0.1199 - val_accuracy: 0.5825 - val_macro_f1: 0.3302 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0236 - accuracy: 0.6838 - macro_f1: 0.5126 - val_loss: 0.1234 - val_accuracy: 0.5848 - val_macro_f1: 0.3356 - lr: 1.0000e-04\n",
      "Epoch 28/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0219 - accuracy: 0.6940 - macro_f1: 0.5234 - val_loss: 0.1330 - val_accuracy: 0.5657 - val_macro_f1: 0.3304 - lr: 1.0000e-04\n",
      "Epoch 29/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0201 - accuracy: 0.6992 - macro_f1: 0.5365 - val_loss: 0.1325 - val_accuracy: 0.5769 - val_macro_f1: 0.3268 - lr: 1.0000e-04\n",
      "Epoch 30/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0175 - accuracy: 0.7103 - macro_f1: 0.5552 - val_loss: 0.1334 - val_accuracy: 0.5788 - val_macro_f1: 0.3341 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0170 - accuracy: 0.7141 - macro_f1: 0.5626 - val_loss: 0.1351 - val_accuracy: 0.5760 - val_macro_f1: 0.3338 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0165 - accuracy: 0.7153 - macro_f1: 0.5697 - val_loss: 0.1360 - val_accuracy: 0.5778 - val_macro_f1: 0.3345 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0163 - accuracy: 0.7190 - macro_f1: 0.5662 - val_loss: 0.1373 - val_accuracy: 0.5778 - val_macro_f1: 0.3354 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0160 - accuracy: 0.7194 - macro_f1: 0.5690 - val_loss: 0.1381 - val_accuracy: 0.5792 - val_macro_f1: 0.3350 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781dd6b226b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_sub_model = create_resnet_model(input_shape, num_classes_sub)\n",
    "train_model(resnet_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 13s 20ms/step - loss: 0.1358 - accuracy: 0.1887 - macro_f1: 0.0567 - val_loss: 0.1775 - val_accuracy: 0.4394 - val_macro_f1: 0.0790 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0944 - accuracy: 0.3520 - macro_f1: 0.1278 - val_loss: 0.1544 - val_accuracy: 0.4436 - val_macro_f1: 0.1646 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0714 - accuracy: 0.4725 - macro_f1: 0.2211 - val_loss: 0.1465 - val_accuracy: 0.5051 - val_macro_f1: 0.1602 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0589 - accuracy: 0.5317 - macro_f1: 0.2824 - val_loss: 0.1336 - val_accuracy: 0.5489 - val_macro_f1: 0.2212 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0497 - accuracy: 0.5588 - macro_f1: 0.3357 - val_loss: 0.1391 - val_accuracy: 0.5312 - val_macro_f1: 0.2180 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0439 - accuracy: 0.5891 - macro_f1: 0.3688 - val_loss: 0.1405 - val_accuracy: 0.5219 - val_macro_f1: 0.2299 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0383 - accuracy: 0.6059 - macro_f1: 0.4028 - val_loss: 0.1452 - val_accuracy: 0.5275 - val_macro_f1: 0.2333 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0344 - accuracy: 0.6327 - macro_f1: 0.4309 - val_loss: 0.1545 - val_accuracy: 0.4888 - val_macro_f1: 0.2459 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0304 - accuracy: 0.6482 - macro_f1: 0.4588 - val_loss: 0.1568 - val_accuracy: 0.5144 - val_macro_f1: 0.2330 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0214 - accuracy: 0.7014 - macro_f1: 0.5249 - val_loss: 0.1467 - val_accuracy: 0.5666 - val_macro_f1: 0.2607 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0193 - accuracy: 0.7177 - macro_f1: 0.5369 - val_loss: 0.1483 - val_accuracy: 0.5676 - val_macro_f1: 0.2638 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0182 - accuracy: 0.7204 - macro_f1: 0.5452 - val_loss: 0.1517 - val_accuracy: 0.5578 - val_macro_f1: 0.2655 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0174 - accuracy: 0.7273 - macro_f1: 0.5561 - val_loss: 0.1538 - val_accuracy: 0.5648 - val_macro_f1: 0.2636 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0168 - accuracy: 0.7291 - macro_f1: 0.5634 - val_loss: 0.1551 - val_accuracy: 0.5596 - val_macro_f1: 0.2673 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781d52123700>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_sub_model = create_vit_model(input_shape, num_classes_sub)\n",
    "train_model(vit_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, classes):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_threshold = (y_pred >= 0.5).astype(int)\n",
    "    report = classification_report(y_test, y_pred_threshold, target_names=classes, zero_division=0, output_dict=True)\n",
    "    print(classification_report(y_test, y_pred_threshold, target_names=classes, zero_division=0))\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.79      0.73      0.76       496\n",
      "         HYP       0.66      0.56      0.61       262\n",
      "          MI       0.78      0.73      0.76       550\n",
      "        NORM       0.86      0.87      0.86       963\n",
      "        STTC       0.74      0.77      0.75       521\n",
      "\n",
      "   micro avg       0.79      0.77      0.78      2792\n",
      "   macro avg       0.77      0.73      0.75      2792\n",
      "weighted avg       0.79      0.77      0.78      2792\n",
      " samples avg       0.78      0.79      0.77      2792\n",
      "\n",
      "ResNet Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.79      0.71      0.75       496\n",
      "         HYP       0.75      0.45      0.56       262\n",
      "          MI       0.75      0.74      0.74       550\n",
      "        NORM       0.83      0.88      0.86       963\n",
      "        STTC       0.74      0.75      0.75       521\n",
      "\n",
      "   micro avg       0.78      0.76      0.77      2792\n",
      "   macro avg       0.77      0.71      0.73      2792\n",
      "weighted avg       0.78      0.76      0.77      2792\n",
      " samples avg       0.78      0.78      0.76      2792\n",
      "\n",
      "ViT Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.74      0.65      0.69       496\n",
      "         HYP       0.71      0.33      0.45       262\n",
      "          MI       0.75      0.53      0.62       550\n",
      "        NORM       0.85      0.77      0.80       963\n",
      "        STTC       0.73      0.64      0.68       521\n",
      "\n",
      "   micro avg       0.78      0.64      0.70      2792\n",
      "   macro avg       0.76      0.58      0.65      2792\n",
      "weighted avg       0.78      0.64      0.69      2792\n",
      " samples avg       0.70      0.67      0.67      2792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN Superdiagnostic Classification Report:\")\n",
    "cnn_super_report = evaluate_model(cnn_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ResNet Superdiagnostic Classification Report:\")\n",
    "resnet_super_report = evaluate_model(resnet_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ViT Superdiagnostic Classification Report:\")\n",
    "vit_super_report = evaluate_model(vit_super_model, X_test, y_test_super, classes_super)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 889us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.85      0.63      0.72       306\n",
      "       CLBBB       0.89      0.89      0.89        54\n",
      "       CRBBB       0.81      0.89      0.85        54\n",
      "       ILBBB       0.08      0.12      0.10         8\n",
      "         IMI       0.73      0.53      0.61       327\n",
      "       IRBBB       0.60      0.61      0.60       112\n",
      "        ISCA       0.49      0.27      0.35        93\n",
      "        ISCI       0.41      0.28      0.33        40\n",
      "        ISC_       0.73      0.45      0.55       128\n",
      "        IVCD       0.17      0.11      0.14        79\n",
      "   LAFB/LPFB       0.81      0.68      0.74       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.20      0.15      0.17        20\n",
      "         LVH       0.76      0.50      0.61       214\n",
      "        NORM       0.87      0.78      0.82       963\n",
      "        NST_       0.21      0.13      0.16        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.83      0.50      0.62        10\n",
      "         RVH       0.33      0.17      0.22        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.49      0.40      0.44       222\n",
      "         WPW       0.80      0.50      0.62         8\n",
      "        _AVB       0.57      0.34      0.43        82\n",
      "\n",
      "   micro avg       0.73      0.58      0.65      3034\n",
      "   macro avg       0.51      0.39      0.43      3034\n",
      "weighted avg       0.71      0.58      0.63      3034\n",
      " samples avg       0.63      0.61      0.61      3034\n",
      "\n",
      "ResNet Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.77      0.54      0.64       306\n",
      "       CLBBB       0.94      0.89      0.91        54\n",
      "       CRBBB       0.85      0.93      0.88        54\n",
      "       ILBBB       0.14      0.12      0.13         8\n",
      "         IMI       0.74      0.50      0.60       327\n",
      "       IRBBB       0.51      0.67      0.58       112\n",
      "        ISCA       0.35      0.26      0.30        93\n",
      "        ISCI       0.40      0.40      0.40        40\n",
      "        ISC_       0.68      0.50      0.58       128\n",
      "        IVCD       0.14      0.15      0.15        79\n",
      "   LAFB/LPFB       0.81      0.72      0.76       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.11      0.05      0.07        20\n",
      "         LVH       0.73      0.57      0.64       214\n",
      "        NORM       0.86      0.75      0.80       963\n",
      "        NST_       0.31      0.27      0.29        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.50      0.40      0.44        10\n",
      "         RVH       0.25      0.08      0.12        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.43      0.42      0.43       222\n",
      "         WPW       1.00      0.25      0.40         8\n",
      "        _AVB       0.44      0.26      0.32        82\n",
      "\n",
      "   micro avg       0.69      0.57      0.63      3034\n",
      "   macro avg       0.48      0.38      0.41      3034\n",
      "weighted avg       0.69      0.57      0.62      3034\n",
      " samples avg       0.63      0.61      0.60      3034\n",
      "\n",
      "ViT Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.71      0.40      0.51       306\n",
      "       CLBBB       0.78      0.87      0.82        54\n",
      "       CRBBB       0.72      0.67      0.69        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.58      0.29      0.39       327\n",
      "       IRBBB       0.39      0.43      0.41       112\n",
      "        ISCA       0.24      0.19      0.21        93\n",
      "        ISCI       0.25      0.03      0.05        40\n",
      "        ISC_       0.68      0.37      0.48       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.73      0.44      0.55       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.63      0.46      0.53       214\n",
      "        NORM       0.80      0.74      0.77       963\n",
      "        NST_       0.21      0.04      0.07        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.46      0.17      0.24       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.67      0.44      0.53      3034\n",
      "   macro avg       0.31      0.22      0.25      3034\n",
      "weighted avg       0.60      0.44      0.50      3034\n",
      " samples avg       0.51      0.49      0.49      3034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN Subdiagnostic Classification Report:\")\n",
    "cnn_sub_report = evaluate_model(cnn_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ResNet Subdiagnostic Classification Report:\")\n",
    "resnet_sub_report = evaluate_model(resnet_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ViT Subdiagnostic Classification Report:\")\n",
    "vit_sub_report = evaluate_model(vit_sub_model, X_test, y_test_sub, classes_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training on LwF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - 1s 1ms/step\n",
      "Working on CNN for LwF Now:\n",
      "Epoch 1/50\n",
      "267/267 [==============================] - 3s 7ms/step - loss: 0.1065 - accuracy: 0.3763 - macro_f1: 0.1305 - val_loss: 0.1617 - val_accuracy: 0.4301 - val_macro_f1: 0.1585 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0846 - accuracy: 0.4832 - macro_f1: 0.2057 - val_loss: 0.1634 - val_accuracy: 0.3495 - val_macro_f1: 0.1759 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0739 - accuracy: 0.5128 - macro_f1: 0.2427 - val_loss: 0.1676 - val_accuracy: 0.3966 - val_macro_f1: 0.1765 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0700 - accuracy: 0.5194 - macro_f1: 0.2667 - val_loss: 0.1206 - val_accuracy: 0.5471 - val_macro_f1: 0.2493 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0657 - accuracy: 0.5283 - macro_f1: 0.2813 - val_loss: 0.1339 - val_accuracy: 0.5275 - val_macro_f1: 0.2386 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0608 - accuracy: 0.5462 - macro_f1: 0.3070 - val_loss: 0.1248 - val_accuracy: 0.5135 - val_macro_f1: 0.2531 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0583 - accuracy: 0.5623 - macro_f1: 0.3088 - val_loss: 0.1142 - val_accuracy: 0.6039 - val_macro_f1: 0.2660 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0536 - accuracy: 0.5741 - macro_f1: 0.3403 - val_loss: 0.1156 - val_accuracy: 0.5624 - val_macro_f1: 0.2720 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0523 - accuracy: 0.5777 - macro_f1: 0.3452 - val_loss: 0.1222 - val_accuracy: 0.5219 - val_macro_f1: 0.2747 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0495 - accuracy: 0.5872 - macro_f1: 0.3636 - val_loss: 0.1263 - val_accuracy: 0.5242 - val_macro_f1: 0.2832 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0472 - accuracy: 0.5959 - macro_f1: 0.3697 - val_loss: 0.1134 - val_accuracy: 0.5741 - val_macro_f1: 0.3031 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0456 - accuracy: 0.6066 - macro_f1: 0.3811 - val_loss: 0.1157 - val_accuracy: 0.5666 - val_macro_f1: 0.2863 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0438 - accuracy: 0.6107 - macro_f1: 0.3929 - val_loss: 0.1153 - val_accuracy: 0.5699 - val_macro_f1: 0.3185 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0417 - accuracy: 0.6202 - macro_f1: 0.4019 - val_loss: 0.1102 - val_accuracy: 0.5881 - val_macro_f1: 0.3128 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0405 - accuracy: 0.6261 - macro_f1: 0.4081 - val_loss: 0.1116 - val_accuracy: 0.6025 - val_macro_f1: 0.3162 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0383 - accuracy: 0.6323 - macro_f1: 0.4182 - val_loss: 0.1138 - val_accuracy: 0.5764 - val_macro_f1: 0.3009 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0351 - accuracy: 0.6455 - macro_f1: 0.4391 - val_loss: 0.1095 - val_accuracy: 0.6118 - val_macro_f1: 0.3171 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0340 - accuracy: 0.6470 - macro_f1: 0.4461 - val_loss: 0.1132 - val_accuracy: 0.5797 - val_macro_f1: 0.3218 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0315 - accuracy: 0.6556 - macro_f1: 0.4602 - val_loss: 0.1199 - val_accuracy: 0.5843 - val_macro_f1: 0.3086 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0300 - accuracy: 0.6670 - macro_f1: 0.4722 - val_loss: 0.1206 - val_accuracy: 0.5853 - val_macro_f1: 0.3061 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0286 - accuracy: 0.6727 - macro_f1: 0.4842 - val_loss: 0.1239 - val_accuracy: 0.5596 - val_macro_f1: 0.3147 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0281 - accuracy: 0.6810 - macro_f1: 0.4885 - val_loss: 0.1228 - val_accuracy: 0.5736 - val_macro_f1: 0.3254 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0217 - accuracy: 0.7047 - macro_f1: 0.5381 - val_loss: 0.1135 - val_accuracy: 0.6123 - val_macro_f1: 0.3304 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0193 - accuracy: 0.7211 - macro_f1: 0.5459 - val_loss: 0.1156 - val_accuracy: 0.6160 - val_macro_f1: 0.3326 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0183 - accuracy: 0.7210 - macro_f1: 0.5526 - val_loss: 0.1158 - val_accuracy: 0.6160 - val_macro_f1: 0.3331 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0175 - accuracy: 0.7294 - macro_f1: 0.5645 - val_loss: 0.1167 - val_accuracy: 0.6188 - val_macro_f1: 0.3327 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0170 - accuracy: 0.7321 - macro_f1: 0.5670 - val_loss: 0.1178 - val_accuracy: 0.6216 - val_macro_f1: 0.3331 - lr: 1.0000e-04\n",
      "Working on ResNet for LwF Now:\n",
      "534/534 [==============================] - 2s 4ms/step\n",
      "Epoch 1/50\n",
      "267/267 [==============================] - 14s 21ms/step - loss: 0.1154 - accuracy: 0.3177 - macro_f1: 0.0643 - val_loss: 0.1582 - val_accuracy: 0.4716 - val_macro_f1: 0.1117 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0990 - accuracy: 0.4145 - macro_f1: 0.1119 - val_loss: 0.2115 - val_accuracy: 0.1580 - val_macro_f1: 0.1004 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0925 - accuracy: 0.4310 - macro_f1: 0.1420 - val_loss: 0.1943 - val_accuracy: 0.2791 - val_macro_f1: 0.0949 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0884 - accuracy: 0.4509 - macro_f1: 0.1585 - val_loss: 0.1687 - val_accuracy: 0.3844 - val_macro_f1: 0.1460 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0842 - accuracy: 0.4735 - macro_f1: 0.1765 - val_loss: 0.1475 - val_accuracy: 0.4329 - val_macro_f1: 0.1851 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0799 - accuracy: 0.4855 - macro_f1: 0.2040 - val_loss: 0.1678 - val_accuracy: 0.3486 - val_macro_f1: 0.1688 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0754 - accuracy: 0.5033 - macro_f1: 0.2196 - val_loss: 0.1598 - val_accuracy: 0.3840 - val_macro_f1: 0.1710 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0710 - accuracy: 0.5124 - macro_f1: 0.2411 - val_loss: 0.1477 - val_accuracy: 0.4320 - val_macro_f1: 0.2018 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0689 - accuracy: 0.5169 - macro_f1: 0.2529 - val_loss: 0.1412 - val_accuracy: 0.5144 - val_macro_f1: 0.2187 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0671 - accuracy: 0.5139 - macro_f1: 0.2615 - val_loss: 0.1505 - val_accuracy: 0.3807 - val_macro_f1: 0.2236 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0639 - accuracy: 0.5332 - macro_f1: 0.2790 - val_loss: 0.1484 - val_accuracy: 0.3812 - val_macro_f1: 0.2519 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0607 - accuracy: 0.5424 - macro_f1: 0.2994 - val_loss: 0.1276 - val_accuracy: 0.5252 - val_macro_f1: 0.2270 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 6s 21ms/step - loss: 0.0566 - accuracy: 0.5591 - macro_f1: 0.3145 - val_loss: 0.1689 - val_accuracy: 0.4119 - val_macro_f1: 0.2110 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0550 - accuracy: 0.5668 - macro_f1: 0.3244 - val_loss: 0.1401 - val_accuracy: 0.5103 - val_macro_f1: 0.2403 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0525 - accuracy: 0.5770 - macro_f1: 0.3379 - val_loss: 0.1428 - val_accuracy: 0.4637 - val_macro_f1: 0.2609 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0521 - accuracy: 0.5764 - macro_f1: 0.3399 - val_loss: 0.2095 - val_accuracy: 0.2703 - val_macro_f1: 0.1822 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0521 - accuracy: 0.5746 - macro_f1: 0.3386 - val_loss: 0.1374 - val_accuracy: 0.4343 - val_macro_f1: 0.2735 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0436 - accuracy: 0.6096 - macro_f1: 0.3762 - val_loss: 0.1082 - val_accuracy: 0.5876 - val_macro_f1: 0.3110 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0408 - accuracy: 0.6237 - macro_f1: 0.3928 - val_loss: 0.1094 - val_accuracy: 0.5899 - val_macro_f1: 0.3182 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0392 - accuracy: 0.6269 - macro_f1: 0.4034 - val_loss: 0.1095 - val_accuracy: 0.5965 - val_macro_f1: 0.3181 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0379 - accuracy: 0.6340 - macro_f1: 0.4134 - val_loss: 0.1100 - val_accuracy: 0.5909 - val_macro_f1: 0.3210 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0369 - accuracy: 0.6371 - macro_f1: 0.4205 - val_loss: 0.1099 - val_accuracy: 0.6053 - val_macro_f1: 0.3208 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0358 - accuracy: 0.6430 - macro_f1: 0.4298 - val_loss: 0.1108 - val_accuracy: 0.5951 - val_macro_f1: 0.3286 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0340 - accuracy: 0.6473 - macro_f1: 0.4386 - val_loss: 0.1102 - val_accuracy: 0.5983 - val_macro_f1: 0.3273 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0335 - accuracy: 0.6487 - macro_f1: 0.4427 - val_loss: 0.1104 - val_accuracy: 0.5927 - val_macro_f1: 0.3280 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0334 - accuracy: 0.6499 - macro_f1: 0.4448 - val_loss: 0.1107 - val_accuracy: 0.5955 - val_macro_f1: 0.3253 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0333 - accuracy: 0.6496 - macro_f1: 0.4435 - val_loss: 0.1110 - val_accuracy: 0.5988 - val_macro_f1: 0.3323 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0332 - accuracy: 0.6490 - macro_f1: 0.4496 - val_loss: 0.1114 - val_accuracy: 0.5941 - val_macro_f1: 0.3308 - lr: 1.0000e-05\n",
      "Working on ViT for LwF Now:\n",
      "534/534 [==============================] - 3s 6ms/step\n",
      "Epoch 1/50\n",
      "267/267 [==============================] - 13s 20ms/step - loss: 0.1290 - accuracy: 0.2076 - macro_f1: 0.0631 - val_loss: 0.1730 - val_accuracy: 0.4129 - val_macro_f1: 0.0648 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0936 - accuracy: 0.3599 - macro_f1: 0.1278 - val_loss: 0.1714 - val_accuracy: 0.3406 - val_macro_f1: 0.1332 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0699 - accuracy: 0.4693 - macro_f1: 0.2268 - val_loss: 0.1612 - val_accuracy: 0.4119 - val_macro_f1: 0.1932 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0586 - accuracy: 0.5186 - macro_f1: 0.2838 - val_loss: 0.1499 - val_accuracy: 0.4525 - val_macro_f1: 0.1882 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0497 - accuracy: 0.5596 - macro_f1: 0.3296 - val_loss: 0.1486 - val_accuracy: 0.5303 - val_macro_f1: 0.2004 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0447 - accuracy: 0.5789 - macro_f1: 0.3656 - val_loss: 0.1481 - val_accuracy: 0.5061 - val_macro_f1: 0.2108 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0412 - accuracy: 0.5961 - macro_f1: 0.3895 - val_loss: 0.1473 - val_accuracy: 0.5242 - val_macro_f1: 0.2226 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0356 - accuracy: 0.6244 - macro_f1: 0.4312 - val_loss: 0.1523 - val_accuracy: 0.5219 - val_macro_f1: 0.2162 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0322 - accuracy: 0.6328 - macro_f1: 0.4537 - val_loss: 0.1542 - val_accuracy: 0.5214 - val_macro_f1: 0.2534 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0277 - accuracy: 0.6612 - macro_f1: 0.4827 - val_loss: 0.1603 - val_accuracy: 0.5266 - val_macro_f1: 0.2374 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0268 - accuracy: 0.6657 - macro_f1: 0.4890 - val_loss: 0.1669 - val_accuracy: 0.5284 - val_macro_f1: 0.2547 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0234 - accuracy: 0.6812 - macro_f1: 0.5184 - val_loss: 0.1806 - val_accuracy: 0.4963 - val_macro_f1: 0.2281 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0172 - accuracy: 0.7223 - macro_f1: 0.5621 - val_loss: 0.1639 - val_accuracy: 0.5657 - val_macro_f1: 0.2596 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0146 - accuracy: 0.7392 - macro_f1: 0.5832 - val_loss: 0.1655 - val_accuracy: 0.5694 - val_macro_f1: 0.2607 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0133 - accuracy: 0.7496 - macro_f1: 0.5910 - val_loss: 0.1660 - val_accuracy: 0.5806 - val_macro_f1: 0.2607 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0127 - accuracy: 0.7558 - macro_f1: 0.6005 - val_loss: 0.1694 - val_accuracy: 0.5750 - val_macro_f1: 0.2636 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0121 - accuracy: 0.7598 - macro_f1: 0.6023 - val_loss: 0.1705 - val_accuracy: 0.5727 - val_macro_f1: 0.2616 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781d14b8cd00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_soft_targets_super = cnn_super_model.predict(X_train)\n",
    "\n",
    "def lwf_loss(y_true, y_pred, old_predictions, T=2):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    dist_loss = tf.keras.losses.KLDivergence()(tf.nn.softmax(old_predictions / T),\n",
    "                                               tf.nn.softmax(y_pred / T))\n",
    "    total_loss = task_loss + dist_loss\n",
    "    return total_loss\n",
    "\n",
    "print(\"Working on CNN for LwF Now:\")\n",
    "cnn_model_lwf = create_cnn_model(input_shape, num_classes_sub)\n",
    "cnn_model_lwf.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: lwf_loss(y_true, y_pred, old_predictions=cnn_soft_targets_super),\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "train_model(cnn_model_lwf, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n",
    "\n",
    "print(\"Working on ResNet for LwF Now:\")\n",
    "resnet_soft_targets_super = resnet_super_model.predict(X_train)\n",
    "resnet_model_lwf = create_resnet_model(input_shape, num_classes_sub)\n",
    "resnet_model_lwf.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: lwf_loss(y_true, y_pred, old_predictions=resnet_soft_targets_super),\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "train_model(resnet_model_lwf, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n",
    "\n",
    "print(\"Working on ViT for LwF Now:\")\n",
    "vit_soft_targets_super = vit_super_model.predict(X_train)\n",
    "vit_model_lwf = create_vit_model(input_shape, num_classes_sub)\n",
    "vit_model_lwf.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: lwf_loss(y_true, y_pred, old_predictions=vit_soft_targets_super),\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "train_model(vit_model_lwf, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training on EwC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWC:\n",
    "    def __init__(self, model, X, y, batch_size=32, exclude_params=[]):\n",
    "        self.model = model\n",
    "        self.params = {}\n",
    "        for p in model.trainable_variables:\n",
    "            if id(p) not in exclude_params:\n",
    "                self.params[id(p)] = p.numpy()\n",
    "        self.fisher = self.compute_fisher(X, y, batch_size, exclude_params)\n",
    "\n",
    "    def compute_fisher(self, X, y, batch_size, exclude_params):\n",
    "        fisher = {}\n",
    "        num_samples = X.shape[0]\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            X_batch = X[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "            y_batch = y[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = self.model(X_batch)\n",
    "                loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_batch, preds))\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            for p, g in zip(self.model.trainable_variables, grads):\n",
    "                if g is not None and id(p) not in exclude_params:\n",
    "                    param_id = id(p)\n",
    "                    if param_id not in fisher:\n",
    "                        fisher[param_id] = np.square(g.numpy())\n",
    "                    else:\n",
    "                        fisher[param_id] += np.square(g.numpy())\n",
    "        for k in fisher.keys():\n",
    "            fisher[k] /= num_batches\n",
    "        return fisher\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0\n",
    "        for p in model.trainable_variables:\n",
    "            param_id = id(p)\n",
    "            if param_id in self.fisher:\n",
    "                fisher = tf.convert_to_tensor(self.fisher[param_id])\n",
    "                loss += tf.reduce_sum(fisher * tf.square(p - self.params[param_id]))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_for_subdiagnostic(base_model, num_classes_sub):\n",
    "    x = base_model.layers[-2].output\n",
    "    outputs = layers.Dense(num_classes_sub, activation='sigmoid', name='output_sub')(x)\n",
    "    new_model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 3s 7ms/step - loss: 0.0812 - accuracy: 0.5834 - macro_f1: 0.2628 - val_loss: 0.1290 - val_accuracy: 0.5545 - val_macro_f1: 0.2542 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0632 - accuracy: 0.5968 - macro_f1: 0.3219 - val_loss: 0.1114 - val_accuracy: 0.6258 - val_macro_f1: 0.3025 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0537 - accuracy: 0.6193 - macro_f1: 0.3539 - val_loss: 0.1055 - val_accuracy: 0.6081 - val_macro_f1: 0.2877 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0493 - accuracy: 0.6280 - macro_f1: 0.3724 - val_loss: 0.1109 - val_accuracy: 0.6118 - val_macro_f1: 0.3109 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0460 - accuracy: 0.6426 - macro_f1: 0.3845 - val_loss: 0.1087 - val_accuracy: 0.6034 - val_macro_f1: 0.3006 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0425 - accuracy: 0.6424 - macro_f1: 0.4035 - val_loss: 0.1089 - val_accuracy: 0.6198 - val_macro_f1: 0.3159 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0404 - accuracy: 0.6506 - macro_f1: 0.4135 - val_loss: 0.1105 - val_accuracy: 0.5904 - val_macro_f1: 0.3225 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0381 - accuracy: 0.6603 - macro_f1: 0.4285 - val_loss: 0.1087 - val_accuracy: 0.6221 - val_macro_f1: 0.3310 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0304 - accuracy: 0.6864 - macro_f1: 0.4735 - val_loss: 0.1039 - val_accuracy: 0.6286 - val_macro_f1: 0.3415 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0285 - accuracy: 0.6923 - macro_f1: 0.4791 - val_loss: 0.1046 - val_accuracy: 0.6319 - val_macro_f1: 0.3403 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0274 - accuracy: 0.7006 - macro_f1: 0.4855 - val_loss: 0.1034 - val_accuracy: 0.6361 - val_macro_f1: 0.3467 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0266 - accuracy: 0.7019 - macro_f1: 0.4923 - val_loss: 0.1046 - val_accuracy: 0.6277 - val_macro_f1: 0.3492 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0260 - accuracy: 0.7032 - macro_f1: 0.5010 - val_loss: 0.1058 - val_accuracy: 0.6314 - val_macro_f1: 0.3486 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0253 - accuracy: 0.7016 - macro_f1: 0.5029 - val_loss: 0.1062 - val_accuracy: 0.6323 - val_macro_f1: 0.3436 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0247 - accuracy: 0.7063 - macro_f1: 0.5127 - val_loss: 0.1077 - val_accuracy: 0.6253 - val_macro_f1: 0.3495 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0239 - accuracy: 0.7099 - macro_f1: 0.5115 - val_loss: 0.1077 - val_accuracy: 0.6179 - val_macro_f1: 0.3509 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0228 - accuracy: 0.7131 - macro_f1: 0.5189 - val_loss: 0.1081 - val_accuracy: 0.6277 - val_macro_f1: 0.3458 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0228 - accuracy: 0.7153 - macro_f1: 0.5218 - val_loss: 0.1078 - val_accuracy: 0.6281 - val_macro_f1: 0.3487 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0226 - accuracy: 0.7138 - macro_f1: 0.5212 - val_loss: 0.1079 - val_accuracy: 0.6300 - val_macro_f1: 0.3516 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0226 - accuracy: 0.7142 - macro_f1: 0.5236 - val_loss: 0.1078 - val_accuracy: 0.6300 - val_macro_f1: 0.3514 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0223 - accuracy: 0.7168 - macro_f1: 0.5244 - val_loss: 0.1081 - val_accuracy: 0.6300 - val_macro_f1: 0.3461 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781d2c14beb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ewc = 1000\n",
    "cnn_sub_model = modify_model_for_subdiagnostic(cnn_super_model, num_classes_sub)\n",
    "exclude_params_cnn = [id(w) for w in cnn_sub_model.layers[-1].trainable_weights]\n",
    "ewc_cnn = EWC(cnn_super_model, X_train, y_train_super, exclude_params=exclude_params_cnn)\n",
    "\n",
    "def ewc_loss_cnn(y_true, y_pred):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    ewc_penalty = ewc_cnn.penalty(cnn_sub_model)\n",
    "    total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "    return total_loss\n",
    "\n",
    "cnn_sub_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=ewc_loss_cnn,\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "\n",
    "train_model(cnn_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 13s 21ms/step - loss: 0.0950 - accuracy: 0.4998 - macro_f1: 0.1967 - val_loss: 0.1524 - val_accuracy: 0.4133 - val_macro_f1: 0.1713 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0744 - accuracy: 0.5530 - macro_f1: 0.2588 - val_loss: 0.1261 - val_accuracy: 0.5685 - val_macro_f1: 0.2224 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0684 - accuracy: 0.5823 - macro_f1: 0.2821 - val_loss: 0.1190 - val_accuracy: 0.5732 - val_macro_f1: 0.2475 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0662 - accuracy: 0.6013 - macro_f1: 0.3006 - val_loss: 0.1382 - val_accuracy: 0.4692 - val_macro_f1: 0.2228 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0632 - accuracy: 0.5835 - macro_f1: 0.3142 - val_loss: 0.1251 - val_accuracy: 0.5289 - val_macro_f1: 0.2613 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0551 - accuracy: 0.6054 - macro_f1: 0.3417 - val_loss: 0.1183 - val_accuracy: 0.5890 - val_macro_f1: 0.2648 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0497 - accuracy: 0.6181 - macro_f1: 0.3616 - val_loss: 0.1155 - val_accuracy: 0.5680 - val_macro_f1: 0.3065 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0485 - accuracy: 0.6260 - macro_f1: 0.3661 - val_loss: 0.1513 - val_accuracy: 0.4632 - val_macro_f1: 0.2650 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0453 - accuracy: 0.6366 - macro_f1: 0.3802 - val_loss: 0.1134 - val_accuracy: 0.5937 - val_macro_f1: 0.3017 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0415 - accuracy: 0.6482 - macro_f1: 0.4005 - val_loss: 0.1243 - val_accuracy: 0.5466 - val_macro_f1: 0.2967 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0389 - accuracy: 0.6548 - macro_f1: 0.4117 - val_loss: 0.1209 - val_accuracy: 0.5541 - val_macro_f1: 0.3119 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0398 - accuracy: 0.6544 - macro_f1: 0.4168 - val_loss: 0.1539 - val_accuracy: 0.4702 - val_macro_f1: 0.2669 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0404 - accuracy: 0.6466 - macro_f1: 0.4111 - val_loss: 0.1132 - val_accuracy: 0.6030 - val_macro_f1: 0.3162 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0328 - accuracy: 0.6714 - macro_f1: 0.4577 - val_loss: 0.1381 - val_accuracy: 0.5666 - val_macro_f1: 0.2880 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0300 - accuracy: 0.6867 - macro_f1: 0.4699 - val_loss: 0.1291 - val_accuracy: 0.6100 - val_macro_f1: 0.3063 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0286 - accuracy: 0.6844 - macro_f1: 0.4798 - val_loss: 0.1321 - val_accuracy: 0.5457 - val_macro_f1: 0.3236 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0260 - accuracy: 0.6970 - macro_f1: 0.4962 - val_loss: 0.1290 - val_accuracy: 0.5652 - val_macro_f1: 0.3203 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0231 - accuracy: 0.7115 - macro_f1: 0.5178 - val_loss: 0.1319 - val_accuracy: 0.6039 - val_macro_f1: 0.3205 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0155 - accuracy: 0.7532 - macro_f1: 0.5721 - val_loss: 0.1232 - val_accuracy: 0.6137 - val_macro_f1: 0.3413 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0122 - accuracy: 0.7683 - macro_f1: 0.5997 - val_loss: 0.1300 - val_accuracy: 0.6263 - val_macro_f1: 0.3414 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0107 - accuracy: 0.7714 - macro_f1: 0.6085 - val_loss: 0.1402 - val_accuracy: 0.6132 - val_macro_f1: 0.3402 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0095 - accuracy: 0.7781 - macro_f1: 0.6142 - val_loss: 0.1502 - val_accuracy: 0.6226 - val_macro_f1: 0.3405 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "267/267 [==============================] - 6s 21ms/step - loss: 0.0084 - accuracy: 0.7861 - macro_f1: 0.6243 - val_loss: 0.1590 - val_accuracy: 0.6156 - val_macro_f1: 0.3346 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781d2c14b430>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_sub_model = modify_model_for_subdiagnostic(resnet_super_model, num_classes_sub)\n",
    "exclude_params_resnet = [id(w) for w in resnet_sub_model.layers[-1].trainable_weights]\n",
    "ewc_resnet = EWC(resnet_super_model, X_train, y_train_super, exclude_params=exclude_params_resnet)\n",
    "def ewc_loss_resnet(y_true, y_pred):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    ewc_penalty = ewc_resnet.penalty(resnet_sub_model)\n",
    "    total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "    return total_loss\n",
    "\n",
    "resnet_sub_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=ewc_loss_resnet,\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "\n",
    "train_model(resnet_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "267/267 [==============================] - 13s 21ms/step - loss: 0.0994 - accuracy: 0.4638 - macro_f1: 0.1645 - val_loss: 0.1423 - val_accuracy: 0.5303 - val_macro_f1: 0.1746 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0741 - accuracy: 0.5198 - macro_f1: 0.2366 - val_loss: 0.1399 - val_accuracy: 0.5294 - val_macro_f1: 0.1717 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0599 - accuracy: 0.5616 - macro_f1: 0.2817 - val_loss: 0.1375 - val_accuracy: 0.5233 - val_macro_f1: 0.2338 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0481 - accuracy: 0.5924 - macro_f1: 0.3453 - val_loss: 0.1407 - val_accuracy: 0.5382 - val_macro_f1: 0.2542 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0412 - accuracy: 0.6121 - macro_f1: 0.3835 - val_loss: 0.1367 - val_accuracy: 0.5620 - val_macro_f1: 0.2629 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0367 - accuracy: 0.6359 - macro_f1: 0.4183 - val_loss: 0.1358 - val_accuracy: 0.5592 - val_macro_f1: 0.2512 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0313 - accuracy: 0.6514 - macro_f1: 0.4527 - val_loss: 0.1485 - val_accuracy: 0.5559 - val_macro_f1: 0.2544 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0280 - accuracy: 0.6726 - macro_f1: 0.4720 - val_loss: 0.1599 - val_accuracy: 0.5233 - val_macro_f1: 0.2525 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0258 - accuracy: 0.6846 - macro_f1: 0.4961 - val_loss: 0.1557 - val_accuracy: 0.5573 - val_macro_f1: 0.2573 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0229 - accuracy: 0.7008 - macro_f1: 0.5132 - val_loss: 0.1601 - val_accuracy: 0.5485 - val_macro_f1: 0.2601 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0209 - accuracy: 0.7081 - macro_f1: 0.5324 - val_loss: 0.1684 - val_accuracy: 0.5457 - val_macro_f1: 0.2520 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0142 - accuracy: 0.7502 - macro_f1: 0.5808 - val_loss: 0.1641 - val_accuracy: 0.5834 - val_macro_f1: 0.2641 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0119 - accuracy: 0.7626 - macro_f1: 0.5999 - val_loss: 0.1673 - val_accuracy: 0.5801 - val_macro_f1: 0.2639 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0111 - accuracy: 0.7707 - macro_f1: 0.6033 - val_loss: 0.1711 - val_accuracy: 0.5829 - val_macro_f1: 0.2625 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0104 - accuracy: 0.7696 - macro_f1: 0.6115 - val_loss: 0.1738 - val_accuracy: 0.5862 - val_macro_f1: 0.2629 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0097 - accuracy: 0.7765 - macro_f1: 0.6181 - val_loss: 0.1757 - val_accuracy: 0.5946 - val_macro_f1: 0.2669 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x781d0c4eaec0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_sub_model = modify_model_for_subdiagnostic(vit_super_model, num_classes_sub)\n",
    "exclude_params_vit = [id(w) for w in vit_sub_model.layers[-1].trainable_weights]\n",
    "ewc_vit = EWC(vit_super_model, X_train, y_train_super, exclude_params=exclude_params_vit)\n",
    "\n",
    "def ewc_loss_vit(y_true, y_pred):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    ewc_penalty = ewc_vit.penalty(vit_sub_model)\n",
    "    total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "    return total_loss\n",
    "\n",
    "vit_sub_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=ewc_loss_vit,\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "\n",
    "train_model(vit_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training on SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SI:\n",
    "    def __init__(self, prev_model, damping_factor=0.1, exclude_params=[]):\n",
    "        self.prev_params = {}\n",
    "        self.omega = {}\n",
    "        self.damping_factor = damping_factor\n",
    "        self.exclude_params = exclude_params\n",
    "        \n",
    "        for var in prev_model.trainable_variables:\n",
    "            if var.name not in self.exclude_params:\n",
    "                self.prev_params[var.name] = var.numpy()\n",
    "                self.omega[var.name] = np.zeros_like(var.numpy())\n",
    "\n",
    "    def accumulate_importance(self, model, grads):\n",
    "        for var, grad in zip(model.trainable_variables, grads):\n",
    "            if grad is not None and var.name in self.omega:\n",
    "                delta_theta = var.numpy() - self.prev_params[var.name]\n",
    "                self.omega[var.name] += np.abs(grad.numpy() * delta_theta)\n",
    "\n",
    "    def update_omega(self, model):\n",
    "        for var in model.trainable_variables:\n",
    "            if var.name in self.omega:\n",
    "                delta_theta = var.numpy() - self.prev_params[var.name]\n",
    "                denom = np.square(delta_theta) + self.damping_factor\n",
    "                # Avoid division by zero\n",
    "                epsilon = 1e-6\n",
    "                denom = np.where(denom <epsilon, epsilon, denom)\n",
    "                self.omega[var.name] = self.omega[var.name] / denom\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0\n",
    "        for var in model.trainable_variables:\n",
    "            if var.name in self.omega:\n",
    "                omega = tf.convert_to_tensor(self.omega[var.name], dtype=var.dtype)\n",
    "                prev_param = tf.convert_to_tensor(self.prev_params[var.name], dtype=var.dtype)\n",
    "                omega = tf.nn.relu(omega)\n",
    "                loss += tf.reduce_sum(omega * tf.square(var - prev_param))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_sub = y_train_sub.shape[1]\n",
    "cnn_sub_model = modify_model_for_subdiagnostic(cnn_super_model, num_classes_sub)\n",
    "\n",
    "exclude_params_cnn = [w.name for w in cnn_sub_model.layers[-1].trainable_weights]\n",
    "si_cnn = SI(cnn_sub_model, exclude_params=exclude_params_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x781d05795630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x781d05795630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 33.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Time: 7.90s, Loss: 0.1089, Macro F1: 0.3072, Val Loss: 0.1002, Val Macro F1: 0.3011\n",
      "\n",
      "CNN Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Time: 7.25s, Loss: 0.0941, Macro F1: 0.3501, Val Loss: 0.1024, Val Macro F1: 0.3097\n",
      "\n",
      "CNN Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Time: 7.25s, Loss: 0.0838, Macro F1: 0.3961, Val Loss: 0.0978, Val Macro F1: 0.3290\n",
      "\n",
      "CNN Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Time: 7.27s, Loss: 0.0799, Macro F1: 0.4255, Val Loss: 0.0969, Val Macro F1: 0.3289\n",
      "\n",
      "CNN Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 37.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Time: 7.15s, Loss: 0.0802, Macro F1: 0.4386, Val Loss: 0.0979, Val Macro F1: 0.3355\n",
      "\n",
      "CNN Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Time: 7.23s, Loss: 0.4300, Macro F1: 0.4474, Val Loss: 0.0987, Val Macro F1: 0.3370\n",
      "\n",
      "CNN Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Time: 7.25s, Loss: 3.2578, Macro F1: 0.4558, Val Loss: 0.0993, Val Macro F1: 0.3377\n",
      "\n",
      "CNN Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Time: 7.23s, Loss: 37.2523, Macro F1: 0.4622, Val Loss: 0.0999, Val Macro F1: 0.3390\n",
      "\n",
      "CNN Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Time: 7.26s, Loss: 364.9479, Macro F1: 0.4656, Val Loss: 0.1004, Val Macro F1: 0.3381\n",
      "\n",
      "CNN Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Time: 7.27s, Loss: 3873.6130, Macro F1: 0.4700, Val Loss: 0.1011, Val Macro F1: 0.3388\n",
      "\n",
      "CNN Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Time: 7.24s, Loss: 36919.3984, Macro F1: 0.4732, Val Loss: 0.1015, Val Macro F1: 0.3408\n",
      "\n",
      "CNN Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Time: 7.27s, Loss: 359300.4062, Macro F1: 0.4750, Val Loss: 0.1020, Val Macro F1: 0.3386\n",
      "\n",
      "CNN Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Time: 7.24s, Loss: 3355676.7500, Macro F1: 0.4803, Val Loss: 0.1025, Val Macro F1: 0.3425\n",
      "\n",
      "CNN Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Time: 7.26s, Loss: 31651508.0000, Macro F1: 0.4822, Val Loss: 0.1027, Val Macro F1: 0.3424\n",
      "\n",
      "CNN Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Time: 7.24s, Loss: 292165504.0000, Macro F1: 0.4880, Val Loss: 0.1031, Val Macro F1: 0.3411\n",
      "\n",
      "CNN Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Time: 7.23s, Loss: 2786338816.0000, Macro F1: 0.4863, Val Loss: 0.1035, Val Macro F1: 0.3426\n",
      "\n",
      "CNN Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 37.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Time: 7.17s, Loss: 25904293888.0000, Macro F1: 0.4873, Val Loss: 0.1039, Val Macro F1: 0.3410\n",
      "\n",
      "CNN Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Time: 7.22s, Loss: 245290090496.0000, Macro F1: 0.4940, Val Loss: 0.1041, Val Macro F1: 0.3404\n",
      "\n",
      "CNN Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Time: 7.23s, Loss: 2304667287552.0000, Macro F1: 0.4942, Val Loss: 0.1046, Val Macro F1: 0.3413\n",
      "\n",
      "CNN Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 37.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Time: 7.18s, Loss: 21753579438080.0000, Macro F1: 0.4942, Val Loss: 0.1048, Val Macro F1: 0.3417\n",
      "\n",
      "CNN Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Time: 7.21s, Loss: 205460581908480.0000, Macro F1: 0.4970, Val Loss: 0.1053, Val Macro F1: 0.3426\n",
      "\n",
      "CNN Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Time: 7.21s, Loss: 1935975433371648.0000, Macro F1: 0.4975, Val Loss: 0.1058, Val Macro F1: 0.3415\n",
      "\n",
      "CNN Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 37.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Time: 7.18s, Loss: 18340886890938368.0000, Macro F1: 0.4975, Val Loss: 0.1062, Val Macro F1: 0.3411\n",
      "\n",
      "CNN Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Time: 7.21s, Loss: 173829025960034304.0000, Macro F1: 0.4982, Val Loss: 0.1063, Val Macro F1: 0.3426\n",
      "\n",
      "CNN Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:07<00:00, 36.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Time: 7.20s, Loss: 1645146334644142080.0000, Macro F1: 0.5036, Val Loss: 0.1067, Val Macro F1: 0.3415\n"
     ]
    }
   ],
   "source": [
    "lambda_si = 1\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "train_macro_f1 = tf.keras.metrics.Mean(name='train_macro_f1')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_macro_f1 = tf.keras.metrics.Mean(name='val_macro_f1')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f'\\nCNN Epoch {epoch+1}/{epochs}')\n",
    "    train_macro_f1.reset_state()\n",
    "    train_loss.reset_state()\n",
    "    \n",
    "    for step in tqdm(range(len(X_train) // batch_size)):\n",
    "        X_batch = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_train_sub[step*batch_size:(step+1)*batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = cnn_sub_model(X_batch, training=True)\n",
    "            task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "            si_penalty = si_cnn.penalty(cnn_sub_model)\n",
    "            total_loss = tf.reduce_mean(task_loss + lambda_si * si_penalty)\n",
    "        \n",
    "        grads = tape.gradient(total_loss, cnn_sub_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, cnn_sub_model.trainable_variables))\n",
    "        si_cnn.accumulate_importance(cnn_sub_model, grads)\n",
    "        \n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        train_macro_f1.update_state(batch_macro_f1)\n",
    "        train_loss.update_state(total_loss)\n",
    "        \n",
    "    si_cnn.update_omega(cnn_sub_model)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    val_macro_f1.reset_state()\n",
    "    val_loss.reset_state()\n",
    "    for step in range(len(X_val) // batch_size):\n",
    "        X_batch = X_val[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_val_sub[step*batch_size:(step+1)*batch_size]\n",
    "        preds = cnn_sub_model(X_batch, training=False)\n",
    "        task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "        total_loss = tf.reduce_mean(task_loss)\n",
    "\n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        val_macro_f1.update_state(batch_macro_f1)\n",
    "        val_loss.update_state(total_loss)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss.result():.4f}, '\n",
    "          f'Macro F1: {train_macro_f1.result():.4f}, '\n",
    "          f'Val Loss: {val_loss.result():.4f}, '\n",
    "          f'Val Macro F1: {val_macro_f1.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_sub_model = modify_model_for_subdiagnostic(resnet_super_model, num_classes_sub)\n",
    "exclude_params_resnet = [w.name for w in resnet_sub_model.layers[-1].trainable_weights]\n",
    "si_resnet = SI(resnet_sub_model, exclude_params=exclude_params_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ResNet Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:50<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Time: 50.24s, Loss: 0.1104, Macro F1: 0.2991, Val Loss: 0.1084, Val Macro F1: 0.2724\n",
      "\n",
      "ResNet Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Time: 45.91s, Loss: 0.1091, Macro F1: 0.3254, Val Loss: 0.1107, Val Macro F1: 0.2569\n",
      "\n",
      "ResNet Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:46<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Time: 46.03s, Loss: 0.0947, Macro F1: 0.3228, Val Loss: 0.1012, Val Macro F1: 0.2818\n",
      "\n",
      "ResNet Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Time: 45.55s, Loss: 0.0863, Macro F1: 0.3775, Val Loss: 0.1011, Val Macro F1: 0.3037\n",
      "\n",
      "ResNet Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Time: 45.96s, Loss: 0.0902, Macro F1: 0.4014, Val Loss: 0.1030, Val Macro F1: 0.3110\n",
      "\n",
      "ResNet Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Time: 45.99s, Loss: 0.5732, Macro F1: 0.4161, Val Loss: 0.1043, Val Macro F1: 0.3127\n",
      "\n",
      "ResNet Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Time: 45.60s, Loss: 4.2793, Macro F1: 0.4251, Val Loss: 0.1053, Val Macro F1: 0.3166\n",
      "\n",
      "ResNet Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Time: 45.91s, Loss: 50.1227, Macro F1: 0.4311, Val Loss: 0.1060, Val Macro F1: 0.3220\n",
      "\n",
      "ResNet Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Time: 45.83s, Loss: 486.2188, Macro F1: 0.4353, Val Loss: 0.1066, Val Macro F1: 0.3236\n",
      "\n",
      "ResNet Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Time: 45.46s, Loss: 5173.1504, Macro F1: 0.4397, Val Loss: 0.1070, Val Macro F1: 0.3254\n",
      "\n",
      "ResNet Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Time: 45.72s, Loss: 48880.5820, Macro F1: 0.4425, Val Loss: 0.1074, Val Macro F1: 0.3261\n",
      "\n",
      "ResNet Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:46<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Time: 46.06s, Loss: 474433.5625, Macro F1: 0.4460, Val Loss: 0.1078, Val Macro F1: 0.3264\n",
      "\n",
      "ResNet Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Time: 45.35s, Loss: 4423563.0000, Macro F1: 0.4484, Val Loss: 0.1080, Val Macro F1: 0.3269\n",
      "\n",
      "ResNet Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Time: 45.91s, Loss: 41674904.0000, Macro F1: 0.4496, Val Loss: 0.1083, Val Macro F1: 0.3280\n",
      "\n",
      "ResNet Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Time: 46.09s, Loss: 384964864.0000, Macro F1: 0.4507, Val Loss: 0.1085, Val Macro F1: 0.3277\n",
      "\n",
      "ResNet Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Time: 45.70s, Loss: 3667652096.0000, Macro F1: 0.4517, Val Loss: 0.1086, Val Macro F1: 0.3284\n",
      "\n",
      "ResNet Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Time: 45.75s, Loss: 34171596800.0000, Macro F1: 0.4527, Val Loss: 0.1088, Val Macro F1: 0.3273\n",
      "\n",
      "ResNet Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:46<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Time: 46.06s, Loss: 323459645440.0000, Macro F1: 0.4537, Val Loss: 0.1090, Val Macro F1: 0.3270\n",
      "\n",
      "ResNet Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Time: 45.33s, Loss: 3035122106368.0000, Macro F1: 0.4544, Val Loss: 0.1091, Val Macro F1: 0.3274\n",
      "\n",
      "ResNet Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Time: 46.11s, Loss: 28665475760128.0000, Macro F1: 0.4554, Val Loss: 0.1093, Val Macro F1: 0.3285\n",
      "\n",
      "ResNet Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Time: 46.15s, Loss: 270114503000064.0000, Macro F1: 0.4563, Val Loss: 0.1094, Val Macro F1: 0.3296\n",
      "\n",
      "ResNet Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Time: 45.52s, Loss: 2553551331000320.0000, Macro F1: 0.4573, Val Loss: 0.1095, Val Macro F1: 0.3295\n",
      "\n",
      "ResNet Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Time: 45.85s, Loss: 24163108050173952.0000, Macro F1: 0.4579, Val Loss: 0.1096, Val Macro F1: 0.3298\n",
      "\n",
      "ResNet Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Time: 46.00s, Loss: 227764366269743104.0000, Macro F1: 0.4593, Val Loss: 0.1097, Val Macro F1: 0.3278\n",
      "\n",
      "ResNet Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:45<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Time: 45.88s, Loss: nan, Macro F1: 0.0112, Val Loss: nan, Val Macro F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "lambda_si = 1.0  \n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_macro_f1 = tf.keras.metrics.Mean(name='train_macro_f1')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_macro_f1 = tf.keras.metrics.Mean(name='val_macro_f1')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f'\\nResNet Epoch {epoch+1}/{epochs}')\n",
    "    train_macro_f1.reset_state()\n",
    "    train_loss.reset_state()\n",
    "    \n",
    "    for step in tqdm(range(len(X_train) // batch_size)):\n",
    "        X_batch = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_train_sub[step*batch_size:(step+1)*batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = resnet_sub_model(X_batch, training=True)\n",
    "            task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "            si_penalty = si_resnet.penalty(resnet_sub_model)\n",
    "            total_loss = tf.reduce_mean(task_loss + lambda_si * si_penalty)\n",
    "        \n",
    "        grads = tape.gradient(total_loss, resnet_sub_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, resnet_sub_model.trainable_variables))\n",
    "        si_resnet.accumulate_importance(resnet_sub_model, grads)\n",
    "    \n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        train_macro_f1.update_state(batch_macro_f1)\n",
    "        train_loss.update_state(total_loss)\n",
    "    \n",
    "    si_resnet.update_omega(resnet_sub_model)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    val_macro_f1.reset_state()\n",
    "    val_loss.reset_state()\n",
    "    for step in range(len(X_val) // batch_size):\n",
    "        X_batch = X_val[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_val_sub[step*batch_size:(step+1)*batch_size]\n",
    "        preds = resnet_sub_model(X_batch, training=False)\n",
    "        task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "        total_loss = tf.reduce_mean(task_loss)\n",
    "\n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        val_macro_f1.update_state(batch_macro_f1)\n",
    "        val_loss.update_state(total_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss.result():.4f}, '\n",
    "          f'Macro F1: {train_macro_f1.result():.4f}, '\n",
    "          f'Val Loss: {val_loss.result():.4f}, '\n",
    "          f'Val Macro F1: {val_macro_f1.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_sub_model = modify_model_for_subdiagnostic(vit_super_model, num_classes_sub)\n",
    "exclude_params_vit = [w.name for w in vit_sub_model.layers[-1].trainable_weights]\n",
    "si_vit = SI(vit_sub_model, exclude_params=exclude_params_vit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ViT Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:41<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Time: 41.57s, Loss: 0.1121, Macro F1: 0.2549, Val Loss: 0.1105, Val Macro F1: 0.2467\n",
      "\n",
      "ViT Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Time: 36.12s, Loss: 0.0991, Macro F1: 0.3213, Val Loss: 0.1211, Val Macro F1: 0.2437\n",
      "\n",
      "ViT Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Time: 36.06s, Loss: 0.0879, Macro F1: 0.3917, Val Loss: 0.1287, Val Macro F1: 0.2480\n",
      "\n",
      "ViT Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Time: 36.05s, Loss: 0.0796, Macro F1: 0.4500, Val Loss: 0.1299, Val Macro F1: 0.2526\n",
      "\n",
      "ViT Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Time: 36.08s, Loss: 0.0757, Macro F1: 0.4816, Val Loss: 0.1310, Val Macro F1: 0.2603\n",
      "\n",
      "ViT Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Time: 36.08s, Loss: 0.3189, Macro F1: 0.5064, Val Loss: 0.1351, Val Macro F1: 0.2620\n",
      "\n",
      "ViT Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Time: 36.30s, Loss: 2.7850, Macro F1: 0.5176, Val Loss: 0.1399, Val Macro F1: 0.2626\n",
      "\n",
      "ViT Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Time: 36.06s, Loss: 29.1105, Macro F1: 0.5313, Val Loss: 0.1434, Val Macro F1: 0.2653\n",
      "\n",
      "ViT Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Time: 36.07s, Loss: 298.0184, Macro F1: 0.5377, Val Loss: 0.1472, Val Macro F1: 0.2651\n",
      "\n",
      "ViT Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Time: 36.03s, Loss: 3092.7156, Macro F1: 0.5481, Val Loss: 0.1508, Val Macro F1: 0.2640\n",
      "\n",
      "ViT Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Time: 36.21s, Loss: 30043.7070, Macro F1: 0.5545, Val Loss: 0.1537, Val Macro F1: 0.2660\n",
      "\n",
      "ViT Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:35<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Time: 35.96s, Loss: 292215.6562, Macro F1: 0.5624, Val Loss: 0.1585, Val Macro F1: 0.2599\n",
      "\n",
      "ViT Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:35<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Time: 35.81s, Loss: 2735755.7500, Macro F1: 0.5686, Val Loss: 0.1602, Val Macro F1: 0.2661\n",
      "\n",
      "ViT Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Time: 36.69s, Loss: 25809170.0000, Macro F1: 0.5717, Val Loss: 0.1640, Val Macro F1: 0.2597\n",
      "\n",
      "ViT Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Time: 36.04s, Loss: 239026096.0000, Macro F1: 0.5749, Val Loss: 0.1673, Val Macro F1: 0.2577\n",
      "\n",
      "ViT Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:35<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Time: 36.01s, Loss: 2280137472.0000, Macro F1: 0.5756, Val Loss: 0.1700, Val Macro F1: 0.2609\n",
      "\n",
      "ViT Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Time: 36.03s, Loss: 21219457024.0000, Macro F1: 0.5808, Val Loss: 0.1722, Val Macro F1: 0.2588\n",
      "\n",
      "ViT Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:35<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Time: 35.84s, Loss: 201637183488.0000, Macro F1: 0.5843, Val Loss: 0.1751, Val Macro F1: 0.2633\n",
      "\n",
      "ViT Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Time: 36.17s, Loss: 1898964844544.0000, Macro F1: 0.5835, Val Loss: 0.1779, Val Macro F1: 0.2610\n",
      "\n",
      "ViT Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Time: 36.39s, Loss: 17936200761344.0000, Macro F1: 0.5874, Val Loss: 0.1813, Val Macro F1: 0.2642\n",
      "\n",
      "ViT Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Time: 36.07s, Loss: 169584434872320.0000, Macro F1: 0.5893, Val Loss: 0.1843, Val Macro F1: 0.2637\n",
      "\n",
      "ViT Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Time: 36.13s, Loss: 1608641748664320.0000, Macro F1: 0.5896, Val Loss: 0.1863, Val Macro F1: 0.2621\n",
      "\n",
      "ViT Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Time: 36.41s, Loss: 15248848666492928.0000, Macro F1: 0.5923, Val Loss: 0.1874, Val Macro F1: 0.2583\n",
      "\n",
      "ViT Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:35<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Time: 35.99s, Loss: 144423755706269696.0000, Macro F1: 0.5932, Val Loss: 0.1902, Val Macro F1: 0.2586\n",
      "\n",
      "ViT Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 266/266 [00:36<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Time: 36.06s, Loss: nan, Macro F1: 0.0180, Val Loss: nan, Val Macro F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "lambda_si = 1.0\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_macro_f1 = tf.keras.metrics.Mean(name='train_macro_f1')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_macro_f1 = tf.keras.metrics.Mean(name='val_macro_f1')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f'\\nViT Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    train_macro_f1.reset_state()\n",
    "    train_loss.reset_state()\n",
    "\n",
    "    for step in tqdm(range(len(X_train) // batch_size)):\n",
    "        X_batch = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_train_sub[step*batch_size:(step+1)*batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = vit_sub_model(X_batch, training=True)\n",
    "            task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "            si_penalty = si_vit.penalty(vit_sub_model)\n",
    "            total_loss = tf.reduce_mean(task_loss + lambda_si * si_penalty)\n",
    "        \n",
    "        grads = tape.gradient(total_loss, vit_sub_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, vit_sub_model.trainable_variables))\n",
    "        \n",
    "        si_vit.accumulate_importance(vit_sub_model, grads)\n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        train_macro_f1.update_state(batch_macro_f1)\n",
    "        train_loss.update_state(total_loss)\n",
    "        \n",
    "    si_vit.update_omega(vit_sub_model)\n",
    "    epoch_time = time.time() - start_time\n",
    "    val_macro_f1.reset_state()\n",
    "    val_loss.reset_state()\n",
    "    for step in range(len(X_val) // batch_size):\n",
    "        X_batch = X_val[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_val_sub[step*batch_size:(step+1)*batch_size]\n",
    "        preds = vit_sub_model(X_batch, training=False)\n",
    "        task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "        total_loss = tf.reduce_mean(task_loss)\n",
    "        \n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        val_macro_f1.update_state(batch_macro_f1)\n",
    "        val_loss.update_state(total_loss)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss.result():.4f}, '\n",
    "          f'Macro F1: {train_macro_f1.result():.4f}, '\n",
    "          f'Val Loss: {val_loss.result():.4f}, '\n",
    "          f'Val Macro F1: {val_macro_f1.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and Publishing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN EWC Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 957us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.79      0.76      0.78       306\n",
      "       CLBBB       0.94      0.87      0.90        54\n",
      "       CRBBB       0.80      0.89      0.84        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.64      0.74      0.68       327\n",
      "       IRBBB       0.73      0.55      0.63       112\n",
      "        ISCA       0.44      0.26      0.32        93\n",
      "        ISCI       0.42      0.28      0.33        40\n",
      "        ISC_       0.70      0.54      0.61       128\n",
      "        IVCD       0.25      0.06      0.10        79\n",
      "   LAFB/LPFB       0.81      0.67      0.73       179\n",
      "     LAO/LAE       0.22      0.05      0.08        42\n",
      "         LMI       0.33      0.05      0.09        20\n",
      "         LVH       0.77      0.59      0.67       214\n",
      "        NORM       0.85      0.87      0.86       963\n",
      "        NST_       0.25      0.04      0.07        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.75      0.30      0.43        10\n",
      "         RVH       0.12      0.08      0.10        12\n",
      "       SEHYP       1.00      0.50      0.67         2\n",
      "        STTC       0.57      0.38      0.46       222\n",
      "         WPW       1.00      0.38      0.55         8\n",
      "        _AVB       0.65      0.39      0.49        82\n",
      "\n",
      "   micro avg       0.75      0.64      0.69      3034\n",
      "   macro avg       0.57      0.40      0.45      3034\n",
      "weighted avg       0.71      0.64      0.67      3034\n",
      " samples avg       0.70      0.68      0.68      3034\n",
      "\n",
      "CNN EWC Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.76      0.62      0.69       496\n",
      "         HYP       0.76      0.44      0.56       262\n",
      "          MI       0.80      0.60      0.68       550\n",
      "        NORM       0.84      0.83      0.84       963\n",
      "        STTC       0.76      0.64      0.69       521\n",
      "\n",
      "   micro avg       0.80      0.68      0.73      2792\n",
      "   macro avg       0.79      0.63      0.69      2792\n",
      "weighted avg       0.80      0.68      0.73      2792\n",
      " samples avg       0.72      0.70      0.69      2792\n",
      "\n",
      "ResNet EWC Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ResNet EWC Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n",
      "ViT EWC Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ViT EWC Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN EWC Subdiagnostic Classification Report:\")\n",
    "cnn_ewc_sub_report = evaluate_model(cnn_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"CNN EWC Superdiagnostic Classification Report:\")\n",
    "cnn_ewc_super_report = evaluate_model(cnn_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ResNet EWC Subdiagnostic Classification Report:\")\n",
    "resnet_ewc_sub_report = evaluate_model(resnet_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ResNet EWC Superdiagnostic Classification Report:\")\n",
    "resnet_ewc_super_report = evaluate_model(resnet_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ViT EWC Subdiagnostic Classification Report:\")\n",
    "vit_ewc_sub_report = evaluate_model(vit_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ViT EWC Superdiagnostic Classification Report:\")\n",
    "vit_ewc_super_report = evaluate_model(vit_super_model, X_test, y_test_super, classes_super)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN SI Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 876us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.79      0.76      0.78       306\n",
      "       CLBBB       0.94      0.87      0.90        54\n",
      "       CRBBB       0.80      0.89      0.84        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.64      0.74      0.68       327\n",
      "       IRBBB       0.73      0.55      0.63       112\n",
      "        ISCA       0.44      0.26      0.32        93\n",
      "        ISCI       0.42      0.28      0.33        40\n",
      "        ISC_       0.70      0.54      0.61       128\n",
      "        IVCD       0.25      0.06      0.10        79\n",
      "   LAFB/LPFB       0.81      0.67      0.73       179\n",
      "     LAO/LAE       0.22      0.05      0.08        42\n",
      "         LMI       0.33      0.05      0.09        20\n",
      "         LVH       0.77      0.59      0.67       214\n",
      "        NORM       0.85      0.87      0.86       963\n",
      "        NST_       0.25      0.04      0.07        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.75      0.30      0.43        10\n",
      "         RVH       0.12      0.08      0.10        12\n",
      "       SEHYP       1.00      0.50      0.67         2\n",
      "        STTC       0.57      0.38      0.46       222\n",
      "         WPW       1.00      0.38      0.55         8\n",
      "        _AVB       0.65      0.39      0.49        82\n",
      "\n",
      "   micro avg       0.75      0.64      0.69      3034\n",
      "   macro avg       0.57      0.40      0.45      3034\n",
      "weighted avg       0.71      0.64      0.67      3034\n",
      " samples avg       0.70      0.68      0.68      3034\n",
      "\n",
      "CNN SI Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 950us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.76      0.62      0.69       496\n",
      "         HYP       0.76      0.44      0.56       262\n",
      "          MI       0.80      0.60      0.68       550\n",
      "        NORM       0.84      0.83      0.84       963\n",
      "        STTC       0.76      0.64      0.69       521\n",
      "\n",
      "   micro avg       0.80      0.68      0.73      2792\n",
      "   macro avg       0.79      0.63      0.69      2792\n",
      "weighted avg       0.80      0.68      0.73      2792\n",
      " samples avg       0.72      0.70      0.69      2792\n",
      "\n",
      "ResNet SI Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ResNet SI Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n",
      "ViT SI Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ViT SI Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN SI Subdiagnostic Classification Report:\")\n",
    "cnn_si_sub_report = evaluate_model(cnn_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"CNN SI Superdiagnostic Classification Report:\")\n",
    "cnn_si_super_report = evaluate_model(cnn_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ResNet SI Subdiagnostic Classification Report:\")\n",
    "resnet_si_sub_report = evaluate_model(resnet_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ResNet SI Superdiagnostic Classification Report:\")\n",
    "resnet_si_super_report = evaluate_model(resnet_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ViT SI Subdiagnostic Classification Report:\")\n",
    "vit_si_sub_report = evaluate_model(vit_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ViT SI Superdiagnostic Classification Report:\")\n",
    "vit_si_super_report = evaluate_model(vit_super_model, X_test, y_test_super, classes_super)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Classification Performance:\n",
      "     Model                 Task  Macro F1-score\n",
      "0      CNN      Superdiagnostic        0.748892\n",
      "1   ResNet      Superdiagnostic        0.732118\n",
      "2      ViT      Superdiagnostic        0.651557\n",
      "3      CNN        Subdiagnostic        0.433339\n",
      "4   ResNet        Subdiagnostic        0.410882\n",
      "5      ViT        Subdiagnostic        0.248612\n",
      "6      CNN    EWC Subdiagnostic        0.451675\n",
      "7   ResNet    EWC Subdiagnostic        0.000000\n",
      "8      ViT    EWC Subdiagnostic        0.000000\n",
      "9      CNN  EWC Superdiagnostic        0.691851\n",
      "10  ResNet  EWC Superdiagnostic        0.000000\n",
      "11     ViT  EWC Superdiagnostic        0.000000\n",
      "12     CNN     SI Subdiagnostic        0.451675\n",
      "13  ResNet     SI Subdiagnostic        0.000000\n",
      "14     ViT     SI Subdiagnostic        0.000000\n",
      "15     CNN   SI Superdiagnostic        0.691851\n",
      "16  ResNet   SI Superdiagnostic        0.000000\n",
      "17     ViT   SI Superdiagnostic        0.000000\n"
     ]
    }
   ],
   "source": [
    "def get_macro_f1(report_dict):\n",
    "    return report_dict['macro avg']['f1-score']\n",
    "\n",
    "results = {\n",
    "    'Model': [],\n",
    "    'Task': [],\n",
    "    'Macro F1-score': []\n",
    "}\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['Superdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_super_report),\n",
    "    get_macro_f1(resnet_super_report),\n",
    "    get_macro_f1(vit_super_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['Subdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_sub_report),\n",
    "    get_macro_f1(resnet_sub_report),\n",
    "    get_macro_f1(vit_sub_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['EWC Subdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_ewc_sub_report),\n",
    "    get_macro_f1(resnet_ewc_sub_report),\n",
    "    get_macro_f1(vit_ewc_sub_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['EWC Superdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_ewc_super_report),\n",
    "    get_macro_f1(resnet_ewc_super_report),\n",
    "    get_macro_f1(vit_ewc_super_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['SI Subdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_si_sub_report),\n",
    "    get_macro_f1(resnet_si_sub_report),\n",
    "    get_macro_f1(vit_si_sub_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['SI Superdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_si_super_report),\n",
    "    get_macro_f1(resnet_si_super_report),\n",
    "    get_macro_f1(vit_si_super_report)\n",
    "])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Classification Performance:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Driven code for Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Federated Learning with Continual Learning (EWC & SI)\n",
      "--- Federated Training for CNN ---\n",
      "Communication Round 1/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 2/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 3/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 4/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 5/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "--- Evaluation for CNN after Federated Learning ---\n",
      "Evaluating Global CNN Model on Superdiagnostic Task:\n",
      "68/68 [==============================] - 0s 692us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.92      0.48      0.63       496\n",
      "         HYP       0.85      0.23      0.36       262\n",
      "          MI       0.85      0.49      0.62       550\n",
      "        NORM       0.76      0.96      0.85       963\n",
      "        STTC       0.82      0.57      0.67       521\n",
      "\n",
      "   micro avg       0.81      0.64      0.71      2792\n",
      "   macro avg       0.84      0.55      0.63      2792\n",
      "weighted avg       0.83      0.64      0.69      2792\n",
      " samples avg       0.74      0.69      0.70      2792\n",
      "\n",
      "Evaluating Global CNN Model on Subdiagnostic Task:\n",
      "68/68 [==============================] - 0s 685us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.34      0.61      0.44       306\n",
      "       CLBBB       0.79      0.48      0.60        54\n",
      "       CRBBB       0.03      1.00      0.05        54\n",
      "       ILBBB       0.00      0.50      0.01         8\n",
      "         IMI       0.20      0.94      0.33       327\n",
      "       IRBBB       0.04      0.71      0.08       112\n",
      "        ISCA       0.04      0.84      0.08        93\n",
      "        ISCI       0.02      0.40      0.03        40\n",
      "        ISC_       0.07      0.93      0.13       128\n",
      "        IVCD       0.04      0.66      0.07        79\n",
      "   LAFB/LPFB       0.14      0.32      0.20       179\n",
      "     LAO/LAE       0.02      0.74      0.03        42\n",
      "         LMI       0.01      0.40      0.03        20\n",
      "         LVH       0.27      0.55      0.36       214\n",
      "        NORM       0.50      0.95      0.66       963\n",
      "        NST_       0.03      0.87      0.07        77\n",
      "         PMI       0.00      0.50      0.00         2\n",
      "     RAO/RAE       0.00      0.30      0.01        10\n",
      "         RVH       0.01      0.08      0.01        12\n",
      "       SEHYP       0.00      0.50      0.00         2\n",
      "        STTC       0.07      0.03      0.04       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.03      0.66      0.06        82\n",
      "\n",
      "   micro avg       0.08      0.72      0.15      3034\n",
      "   macro avg       0.12      0.56      0.14      3034\n",
      "weighted avg       0.27      0.72      0.36      3034\n",
      " samples avg       0.08      0.73      0.15      3034\n",
      "\n",
      "--- Federated Training for ResNet ---\n",
      "Communication Round 1/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 2/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 3/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 4/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 5/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "--- Evaluation for ResNet after Federated Learning ---\n",
      "Evaluating Global ResNet Model on Superdiagnostic Task:\n",
      "68/68 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.85      0.56      0.68       496\n",
      "         HYP       0.70      0.52      0.60       262\n",
      "          MI       0.71      0.63      0.67       550\n",
      "        NORM       0.82      0.87      0.84       963\n",
      "        STTC       0.79      0.68      0.73       521\n",
      "\n",
      "   micro avg       0.79      0.70      0.74      2792\n",
      "   macro avg       0.78      0.65      0.70      2792\n",
      "weighted avg       0.79      0.70      0.73      2792\n",
      " samples avg       0.74      0.72      0.71      2792\n",
      "\n",
      "Evaluating Global ResNet Model on Subdiagnostic Task:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.16      1.00      0.28       306\n",
      "       CLBBB       0.03      0.98      0.06        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.75      0.01         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.05      1.00      0.10       112\n",
      "        ISCA       0.06      0.42      0.10        93\n",
      "        ISCI       0.12      0.42      0.19        40\n",
      "        ISC_       0.06      0.93      0.11       128\n",
      "        IVCD       0.03      0.08      0.04        79\n",
      "   LAFB/LPFB       0.09      0.99      0.16       179\n",
      "     LAO/LAE       0.01      0.19      0.01        42\n",
      "         LMI       0.01      0.60      0.01        20\n",
      "         LVH       0.13      0.83      0.23       214\n",
      "        NORM       0.35      0.27      0.30       963\n",
      "        NST_       0.04      0.92      0.07        77\n",
      "         PMI       0.00      1.00      0.00         2\n",
      "     RAO/RAE       0.00      0.10      0.00        10\n",
      "         RVH       0.01      0.92      0.01        12\n",
      "       SEHYP       0.00      1.00      0.00         2\n",
      "        STTC       0.06      0.32      0.10       222\n",
      "         WPW       0.00      0.25      0.01         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.05      0.48      0.09      3034\n",
      "   macro avg       0.05      0.56      0.08      3034\n",
      "weighted avg       0.16      0.48      0.18      3034\n",
      " samples avg       0.05      0.43      0.09      3034\n",
      "\n",
      "--- Federated Training for ViT ---\n",
      "Communication Round 1/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 2/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 3/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 4/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "Communication Round 5/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "--- Evaluation for ViT after Federated Learning ---\n",
      "Evaluating Global ViT Model on Superdiagnostic Task:\n",
      "68/68 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.87      0.48      0.62       496\n",
      "         HYP       0.78      0.29      0.42       262\n",
      "          MI       0.77      0.47      0.58       550\n",
      "        NORM       0.76      0.92      0.83       963\n",
      "        STTC       0.79      0.57      0.66       521\n",
      "\n",
      "   micro avg       0.78      0.63      0.70      2792\n",
      "   macro avg       0.79      0.55      0.62      2792\n",
      "weighted avg       0.79      0.63      0.67      2792\n",
      " samples avg       0.71      0.67      0.67      2792\n",
      "\n",
      "Evaluating Global ViT Model on Subdiagnostic Task:\n",
      "68/68 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.12      0.04      0.06       306\n",
      "       CLBBB       0.02      0.78      0.04        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.23      0.61      0.33       327\n",
      "       IRBBB       0.06      0.73      0.11       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.02      1.00      0.04        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.04      0.95      0.07        79\n",
      "   LAFB/LPFB       0.05      0.01      0.01       179\n",
      "     LAO/LAE       0.02      0.93      0.04        42\n",
      "         LMI       0.01      0.90      0.02        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.57      0.83      0.67       963\n",
      "        NST_       0.04      0.58      0.07        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.50      0.01        12\n",
      "       SEHYP       0.00      1.00      0.00         2\n",
      "        STTC       0.03      0.03      0.03       222\n",
      "         WPW       0.00      1.00      0.01         8\n",
      "        _AVB       0.02      0.39      0.05        82\n",
      "\n",
      "   micro avg       0.06      0.47      0.11      3034\n",
      "   macro avg       0.05      0.45      0.07      3034\n",
      "weighted avg       0.23      0.47      0.27      3034\n",
      " samples avg       0.06      0.54      0.11      3034\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 227\u001b[0m\n\u001b[1;32m    224\u001b[0m federated_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(federated_reports)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Append federated results to the existing summary table\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(federated_df, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated Summary of Classification Performance with Federated Learning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# Federated Learning with Continual Learning (EWC & SI)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Starting Federated Learning with Continual Learning (EWC & SI)\")\n",
    "\n",
    "# Number of clients to simulate\n",
    "num_clients = 5\n",
    "\n",
    "# Communication rounds\n",
    "communication_rounds = 5\n",
    "\n",
    "# Local training epochs per client\n",
    "local_epochs = 1\n",
    "\n",
    "# Batch size for local training\n",
    "local_batch_size = 64\n",
    "\n",
    "# Lambda values for EWC and SI\n",
    "lambda_ewc = 1000\n",
    "lambda_si = 1.0\n",
    "\n",
    "# Function to split data among clients\n",
    "def split_data(X, y_super, y_sub, num_clients):\n",
    "    client_data = []\n",
    "    data_per_client = len(X) // num_clients\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * data_per_client\n",
    "        end_idx = (i + 1) * data_per_client if i < num_clients - 1 else len(X)\n",
    "        X_client = X[start_idx:end_idx]\n",
    "        y_client_super = y_super[start_idx:end_idx]\n",
    "        y_client_sub = y_sub[start_idx:end_idx]\n",
    "        client_data.append((X_client, y_client_super, y_client_sub))\n",
    "    return client_data\n",
    "\n",
    "# Split the training data among clients\n",
    "client_data = split_data(X_train, y_train_super, y_train_sub, num_clients)\n",
    "\n",
    "# Function to clone a model and set weights\n",
    "def clone_model_weights(model):\n",
    "    cloned_model = tf.keras.models.clone_model(model)\n",
    "    cloned_model.set_weights(model.get_weights())\n",
    "    return cloned_model\n",
    "\n",
    "# Function to average client weights\n",
    "def average_weights(client_weights):\n",
    "    avg_weights = []\n",
    "    for weights in zip(*client_weights):\n",
    "        avg = np.mean(weights, axis=0)\n",
    "        avg_weights.append(avg)\n",
    "    return avg_weights\n",
    "\n",
    "# Define Federated Learning with EWC and SI\n",
    "def federated_training(model_type, create_model_fn, input_shape, num_classes_super, num_classes_sub, classes_super, classes_sub):\n",
    "    print(f\"--- Federated Training for {model_type} ---\")\n",
    "    \n",
    "    # Initialize the global model\n",
    "    global_model_super = create_model_fn(input_shape, num_classes_super)\n",
    "    global_weights_super = global_model_super.get_weights()\n",
    "    \n",
    "    # Initialize the global models for subdiagnostic tasks\n",
    "    global_model_sub = modify_model_for_subdiagnostic(global_model_super, num_classes_sub)\n",
    "    global_weights_sub = global_model_sub.get_weights()\n",
    "    \n",
    "    for round_num in range(communication_rounds):\n",
    "        print(f\"Communication Round {round_num+1}/{communication_rounds}\")\n",
    "        client_weights_super = []\n",
    "        client_weights_sub = []\n",
    "        \n",
    "        for client_idx, (X_client, y_client_super, y_client_sub) in enumerate(client_data):\n",
    "            print(f\" - Client {client_idx+1}/{num_clients} local training\")\n",
    "            \n",
    "            # Clone global models\n",
    "            client_model_super = clone_model_weights(global_model_super)\n",
    "            client_model_sub = clone_model_weights(global_model_sub)\n",
    "            \n",
    "            # --------------------\n",
    "            # Local Training on Superdiagnostic Task\n",
    "            # --------------------\n",
    "            client_model_super.compile(\n",
    "                optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=[macro_f1]\n",
    "            )\n",
    "            client_model_super.fit(\n",
    "                X_client, y_client_super,\n",
    "                epochs=local_epochs,\n",
    "                batch_size=local_batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Initialize EWC for the client\n",
    "            exclude_params_super = [id(w) for w in client_model_super.layers[-1].trainable_weights]\n",
    "            ewc_client = EWC(client_model_super, X_client, y_client_super, exclude_params=exclude_params_super)\n",
    "            \n",
    "            # --------------------\n",
    "            # Local Training on Subdiagnostic Task with EWC\n",
    "            # --------------------\n",
    "            client_model_sub = modify_model_for_subdiagnostic(client_model_super, num_classes_sub)\n",
    "            \n",
    "            # Define EWC loss for subdiagnostic task\n",
    "            def ewc_loss_sub(y_true, y_pred):\n",
    "                task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "                ewc_penalty = ewc_client.penalty(client_model_sub)\n",
    "                total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "                return total_loss\n",
    "            \n",
    "            client_model_sub.compile(\n",
    "                optimizer='adam',\n",
    "                loss=ewc_loss_sub,\n",
    "                metrics=[macro_f1]\n",
    "            )\n",
    "            client_model_sub.fit(\n",
    "                X_client, y_client_sub,\n",
    "                epochs=local_epochs,\n",
    "                batch_size=local_batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Initialize SI for the client\n",
    "            exclude_params_sub = [id(w) for w in client_model_sub.layers[-1].trainable_weights]\n",
    "            si_client = SI(client_model_sub, damping_factor=0.1, exclude_params=exclude_params_sub)\n",
    "            \n",
    "            # --------------------\n",
    "            # Local Training on Subdiagnostic Task with SI\n",
    "            # --------------------\n",
    "            # Define SI loss for subdiagnostic task\n",
    "            def si_loss_sub(y_true, y_pred):\n",
    "                task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "                si_penalty = si_client.penalty(client_model_sub)\n",
    "                total_loss = task_loss + (lambda_si / 2) * si_penalty\n",
    "                return total_loss\n",
    "            \n",
    "            client_model_sub.compile(\n",
    "                optimizer='adam',\n",
    "                loss=si_loss_sub,\n",
    "                metrics=[macro_f1]\n",
    "            )\n",
    "            client_model_sub.fit(\n",
    "                X_client, y_client_sub,\n",
    "                epochs=local_epochs,\n",
    "                batch_size=local_batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Update EWC and SI\n",
    "            # For EWC, we've already computed the penalty during training\n",
    "            # For SI, update omega after training\n",
    "            si_client.update_omega(client_model_sub)\n",
    "            \n",
    "            # Collect client weights\n",
    "            client_weights_super.append(client_model_super.get_weights())\n",
    "            client_weights_sub.append(client_model_sub.get_weights())\n",
    "        \n",
    "        # Aggregate client weights to update global models\n",
    "        global_weights_super = average_weights(client_weights_super)\n",
    "        global_weights_sub = average_weights(client_weights_sub)\n",
    "        \n",
    "        # Set the aggregated weights to global models\n",
    "        global_model_super.set_weights(global_weights_super)\n",
    "        global_model_sub.set_weights(global_weights_sub)\n",
    "    \n",
    "    # After federated training, evaluate the global models\n",
    "    print(f\"--- Evaluation for {model_type} after Federated Learning ---\")\n",
    "    \n",
    "    # Evaluate on Superdiagnostic Task\n",
    "    print(f\"Evaluating Global {model_type} Model on Superdiagnostic Task:\")\n",
    "    super_report = evaluate_model(global_model_super, X_test, y_test_super, classes_super)\n",
    "    \n",
    "    # Evaluate on Subdiagnostic Task\n",
    "    print(f\"Evaluating Global {model_type} Model on Subdiagnostic Task:\")\n",
    "    global_model_sub = modify_model_for_subdiagnostic(global_model_super, num_classes_sub)\n",
    "    sub_report = evaluate_model(global_model_sub, X_test, y_test_sub, classes_sub)\n",
    "    \n",
    "    return super_report, sub_report\n",
    "\n",
    "# ----------------------------------------\n",
    "# Start Federated Training for CNN, ResNet, ViT\n",
    "# ----------------------------------------\n",
    "\n",
    "# Initialize a dictionary to store federated reports\n",
    "federated_reports = {\n",
    "    'Model': [],\n",
    "    'Task': [],\n",
    "    'Macro F1-score': []\n",
    "}\n",
    "\n",
    "# List of models to federate\n",
    "models_to_federate = [\n",
    "    ('CNN', create_cnn_model),\n",
    "    ('ResNet', create_resnet_model),\n",
    "    ('ViT', create_vit_model)\n",
    "]\n",
    "\n",
    "for model_name, create_model_fn in models_to_federate:\n",
    "    # Determine number of classes based on model type\n",
    "    if model_name in ['CNN', 'ResNet', 'ViT']:\n",
    "        num_classes_sub = y_train_sub.shape[1]\n",
    "    \n",
    "    # Federated training\n",
    "    super_report, sub_report = federated_training(\n",
    "        model_type=model_name,\n",
    "        create_model_fn=create_model_fn,\n",
    "        input_shape=input_shape,\n",
    "        num_classes_super=num_classes_super,\n",
    "        num_classes_sub=num_classes_sub,\n",
    "        classes_super=classes_super,\n",
    "        classes_sub=classes_sub\n",
    "    )\n",
    "    \n",
    "    # Store the reports\n",
    "    federated_reports['Model'].extend([model_name, model_name])\n",
    "    federated_reports['Task'].extend(['Federated Superdiagnostic', 'Federated Subdiagnostic'])\n",
    "    federated_reports['Macro F1-score'].extend([\n",
    "        get_macro_f1(super_report),\n",
    "        get_macro_f1(sub_report)\n",
    "    ])\n",
    "\n",
    "# ----------------------------------------\n",
    "# Update and Publish the Summary Table with Federated Results\n",
    "# ----------------------------------------\n",
    "\n",
    "# Convert federated reports to DataFrame\n",
    "federated_df = pd.DataFrame(federated_reports)\n",
    "\n",
    "# Append federated results to the existing summary table\n",
    "results = results.append(federated_df, ignore_index=True)\n",
    "\n",
    "print(\"Updated Summary of Classification Performance with Federated Learning:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
